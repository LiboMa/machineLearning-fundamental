{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inputs = np.array([[1,2],[1,3],[1,4],[1,5],[5,2],[3,1],[2,1],[1,1]])\n",
    "test_set = [[6,1],[-1, -2],[-2, 3],[3, 2],[3,2], [7,1], [-1, -2],[1,1]]\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 0]])\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 0]]).reshape(y.shape[1],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neuron):\n",
    "        self.layer_n = None\n",
    "        self.n_neuron = n_neuron\n",
    "        self.weights = np.random.randn(n_inputs, n_neuron)\n",
    "        self.biases = np.zeros((1, n_neuron))\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases   \n",
    "    def __repr__(self):\n",
    "        return \"layer: {}th, neurons:{}, weights: {}, biases:{}\\n\".format(self.layer_n, self.n_neuron, self.weights, self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "\n",
    "def activate_sigmoid(inputs):\n",
    "    return 1.0/(1.0+np.exp(-inputs))\n",
    "\n",
    "def der_sigmoid(inputs):\n",
    "    return inputs * (1 - inputs)\n",
    "\n",
    "def activate_ReLU(inputs):\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "def der_active_ReLU(inputs):\n",
    "    inputs[inputs > 0 ] = 1\n",
    "    inputs[inputs <= 0 ] = 0\n",
    "    return inputs\n",
    "\n",
    "def loss_function_mse(predict, target):\n",
    "    return np.mean((predict - target)**2)\n",
    "def der_loss_function_mse(predict, target):\n",
    "    return predict - target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial network model\n",
    "def build_network(network_list=[2,1,2]):\n",
    "    \n",
    "    # network_list = [2,1,2], 2, inputs, 1, hidden1-1, output, 2\n",
    "    network = []\n",
    "    if not isinstance(network_list, list):\n",
    "        return None\n",
    "    \n",
    "    # build network\n",
    "    for i in range(len(network_list)-1 ):\n",
    "        layer = LayerDense(network_list[i],network_list[i+1])\n",
    "        layer.layer_n = i+1\n",
    "        network.append(layer)\n",
    "    \n",
    "    return network\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[layer: 1th, neurons:4, weights: [[-0.12242003 -0.08985308 -0.55628707  0.46057309]\n",
       "  [-1.56935643  0.47515891  1.07711821  0.6684586 ]], biases:[[0. 0. 0. 0.]],\n",
       " layer: 2th, neurons:2, weights: [[ 1.40338928  1.06596159]\n",
       "  [-1.91902697 -1.60496406]\n",
       "  [ 2.09128109  1.13869029]\n",
       "  [ 1.48799034  0.38014224]], biases:[[0. 0.]],\n",
       " layer: 3th, neurons:1, weights: [[2.19830854]\n",
       "  [1.47979871]], biases:[[0.]]]"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build network with Weights\n",
    "networks = [2,4,2,1]\n",
    "network = build_network(networks)\n",
    "\n",
    "network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation over network\n",
    "\n",
    "# layer1 = network[0]\n",
    "# z1 = np.dot(X_inputs, layer1.weights) + layer1.biases\n",
    "# a = activate_sigmoid(z1)\n",
    "\n",
    "# output = [a]\n",
    "output = [X_inputs]\n",
    "for layer in range(len(network)):\n",
    "#     z = np.dot(output[-1], network[layer].weights) + network[layer].biases\n",
    "    network[layer].forward(output[-1])\n",
    "    z = network[layer].output\n",
    "    a = activate_sigmoid(z)\n",
    "#     print(output)\n",
    "    output.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93536619],\n",
       "       [0.93632828],\n",
       "       [0.93517063],\n",
       "       [0.93339357],\n",
       "       [0.90694033],\n",
       "       [0.91830126],\n",
       "       [0.92604844],\n",
       "       [0.93210535]])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes (8, 2)\n",
      "shapes (8, 4)\n",
      "shapes (8, 2)\n",
      "shapes (8, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ print(\"shapes\", o.shape) for o in output ]\n",
    "# X =  np.round(np.random.randn(20,2),3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back-propagation for last layer\n",
    "lr = 0.01\n",
    "error = output[-1] - y\n",
    "\n",
    "# delta = output.T.dot(error)\n",
    "network[-1].biases = network[-1].biases - error.mean() * lr\n",
    "network[-1].weights = network[-1].weights - (output[-1].T.dot(error)) * lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[layer: 1th, neurons:4, weights: [[-0.12242003 -0.08985308 -0.55628707  0.46057309]\n",
       "  [-1.56935643  0.47515891  1.07711821  0.6684586 ]], biases:[[0. 0. 0. 0.]],\n",
       " layer: 2th, neurons:2, weights: [[ 1.40338928  1.06596159]\n",
       "  [-1.91902697 -1.60496406]\n",
       "  [ 2.09128109  1.13869029]\n",
       "  [ 1.48799034  0.38014224]], biases:[[0. 0.]],\n",
       " layer: 3th, neurons:1, weights: [[2.15692551]\n",
       "  [1.43841569]], biases:[[-0.00552957]]]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first run delta []\n",
      "next a [[0.93536619]\n",
      " [0.93632828]\n",
      " [0.93517063]\n",
      " [0.93339357]\n",
      " [0.90694033]\n",
      " [0.91830126]\n",
      " [0.92604844]\n",
      " [0.93210535]]\n",
      "output Matrix shape (8, 2), delta Matrix shape (8, 1)\n",
      "===> output.matrix[[0.8480079  0.54603202]\n",
      " [0.86052693 0.53826371]\n",
      " [0.86083779 0.52478973]\n",
      " [0.85677952 0.51125875]\n",
      " [0.7340403  0.44816145]\n",
      " [0.7653693  0.4980198 ]\n",
      " [0.79566119 0.5260225 ]\n",
      " [0.8194369  0.55285456]] dot delta[-1] [[ 0.05654876]\n",
      " [ 0.05582167]\n",
      " [ 0.05669615]\n",
      " [ 0.05802909]\n",
      " [-0.0078542 ]\n",
      " [-0.00612937]\n",
      " [-0.00506441]\n",
      " [ 0.05898826]].matrix\n",
      "===> network 2 layer, weights => [[2.14550725]\n",
      " [1.43122963]] bias =>[[-0.00719854]] updated\n",
      "currnet delta is: --> [[ 0.05654876]\n",
      " [ 0.05582167]\n",
      " [ 0.05669615]\n",
      " [ 0.05802909]\n",
      " [-0.0078542 ]\n",
      " [-0.00612937]\n",
      " [-0.00506441]\n",
      " [ 0.05898826]]\n",
      "previous weights is: --> [[2.15692551 1.43841569]]\n",
      "output Matrix shape (8, 4), delta Matrix shape (8, 2)\n",
      "===> output.matrix[[3.69288965e-02 7.02757742e-01 8.31731583e-01 8.57843155e-01]\n",
      " [7.91939670e-03 7.91769329e-01 9.35539308e-01 9.21719967e-01]\n",
      " [1.65905653e-03 8.59456425e-01 9.77071648e-01 9.58289919e-01]\n",
      " [3.45835237e-04 9.07706004e-01 9.92071285e-01 9.78179987e-01]\n",
      " [2.29591253e-02 6.22706623e-01 3.48145941e-01 9.74413793e-01]\n",
      " [1.26020038e-01 5.51219619e-01 3.56235028e-01 8.85965589e-01]\n",
      " [1.40131706e-01 5.73330530e-01 4.91136945e-01 8.30560490e-01]\n",
      " [1.55542361e-01 5.95152164e-01 6.27342094e-01 7.55660157e-01]] dot delta[-1] [[ 0.01572096  0.0201628 ]\n",
      " [ 0.01445083  0.01995613]\n",
      " [ 0.01464979  0.02033804]\n",
      " [ 0.01535872  0.02085691]\n",
      " [-0.00330729 -0.00279404]\n",
      " [-0.00237414 -0.00220411]\n",
      " [-0.001776   -0.00181625]\n",
      " [ 0.01882544  0.02097537]].matrix\n",
      "===> network 1 layer, weights => [[ 1.40323784  1.0657811 ]\n",
      " [-1.92181894 -1.60870752]\n",
      " [ 2.08802678  1.1343647 ]\n",
      " [ 1.48482572  0.37587982]] bias =>[[-0.00044718 -0.00059672]] updated\n",
      "currnet delta is: --> [[ 0.01572096  0.0201628 ]\n",
      " [ 0.01445083  0.01995613]\n",
      " [ 0.01464979  0.02033804]\n",
      " [ 0.01535872  0.02085691]\n",
      " [-0.00330729 -0.00279404]\n",
      " [-0.00237414 -0.00220411]\n",
      " [-0.001776   -0.00181625]\n",
      " [ 0.01882544  0.02097537]]\n",
      "previous weights is: --> [[ 1.40338928 -1.91902697  2.09128109  1.48799034]\n",
      " [ 1.06596159 -1.60496406  1.13869029  0.38014224]]\n",
      "output Matrix shape (8, 2), delta Matrix shape (8, 4)\n",
      "===> output.matrix[[1 2]\n",
      " [1 3]\n",
      " [1 4]\n",
      " [1 5]\n",
      " [5 2]\n",
      " [3 1]\n",
      " [2 1]\n",
      " [1 1]] dot delta[-1] [[ 1.54905446e-03 -1.30617473e-02  7.81449923e-03  3.78739298e-03]\n",
      " [ 3.26465576e-04 -9.85273827e-03  3.19285145e-03  2.09882772e-03]\n",
      " [ 6.99605005e-05 -7.33868123e-03  1.20516251e-03  1.18032805e-03]\n",
      " [ 1.51378273e-05 -5.27355309e-03  4.39456965e-04  6.57012749e-04]\n",
      " [-1.70926446e-04  2.54469613e-03 -2.29164916e-03 -1.49174254e-04]\n",
      " [-6.25737602e-04  2.00215801e-03 -1.71420831e-03 -4.41561962e-04]\n",
      " [-5.33607285e-04  1.54679792e-03 -1.44510956e-03 -4.69066510e-04]\n",
      " [ 6.40698654e-03 -1.68159277e-02  1.47877135e-02  6.64432415e-03]].matrix\n",
      "===> network 0 layer, weights => [[-0.12264846 -0.08832712 -0.5566845   0.46000513]\n",
      " [-1.56982337  0.48113799  1.07515468  0.66709295]] bias =>[[-4.39833348e-05  2.89056222e-04 -1.37429479e-04 -8.31755183e-05]] updated\n"
     ]
    }
   ],
   "source": [
    "# Back-propagation\n",
    "lr = 0.05\n",
    "# back = list(range(len(output)-1))\n",
    "# back.reverse()\n",
    "# back-propagation for rest of layers and updates weights and biases\n",
    "delta = []\n",
    "back = list(range(len(output)-1))\n",
    "back.reverse()\n",
    "\n",
    "for i in back:\n",
    "    # get one of the output of next layer\n",
    "    next_a = output[i+1]\n",
    "#     print(next_a)\n",
    "    if i == back[0]:\n",
    "        print(\"first run delta\",delta)\n",
    "        # calculate the delta between current and next layer\n",
    "        # error * der_sigmoid(previous_output), gredicent here\n",
    "        print(\"next a\", next_a)\n",
    "        x = (next_a - y) * der_sigmoid(next_a)\n",
    "        delta.append(x)\n",
    "#         print(\"delta current is: -->{}\\n\".format(delta))\n",
    "    else:\n",
    "        # delta = last delta.T.dot(last weights)*der_sigmoid_with_output\n",
    "        print(\"currnet delta is: -->\", delta[-1])\n",
    "        print(\"previous weights is: -->\", previous_W)\n",
    "\n",
    "        x = delta[-1] @ previous_W * der_sigmoid(next_a)\n",
    "        delta.append(x)\n",
    "    \n",
    "    # store tmp W for next layer previous compute\n",
    "    previous_W = network[i].weights.transpose()\n",
    "    \n",
    "    # gradicent descent for update weights\n",
    "    network[i].biases = network[i].biases - np.mean(delta[-1], axis = 0, keepdims = True) * lr\n",
    "    print(\"output Matrix shape {}, delta Matrix shape {}\".format(output[i].shape, delta[-1].shape))\n",
    "    print(\"===> output.matrix{} dot delta[-1] {}.matrix\".format(output[i], delta[-1]))\n",
    "    network[i].weights = network[i].weights - output[i].T @ delta[-1] * lr\n",
    "    \n",
    "    print(\"===> network {} layer, weights => {} bias =>{} updated\".format(i, network[i].weights, network[i].biases))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[layer: 1th, neurons:4, weights: [[-0.12264846 -0.08832712 -0.5566845   0.46000513]\n",
      " [-1.56982337  0.48113799  1.07515468  0.66709295]], biases:[[-4.39833348e-05  2.89056222e-04 -1.37429479e-04 -8.31755183e-05]]\n",
      ",\n",
      " layer: 2th, neurons:2, weights: [[ 1.40323784  1.0657811 ]\n",
      " [-1.92181894 -1.60870752]\n",
      " [ 2.08802678  1.1343647 ]\n",
      " [ 1.48482572  0.37587982]], biases:[[-0.00044718 -0.00059672]]\n",
      ",\n",
      " layer: 3th, neurons:1, weights: [[2.14550725]\n",
      " [1.43122963]], biases:[[-0.00719854]]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "pprint(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5483765272684813\n",
      "Estimation: [[0.93536619]\n",
      " [0.93632828]\n",
      " [0.93517063]\n",
      " [0.93339357]\n",
      " [0.90694033]\n",
      " [0.91830126]\n",
      " [0.92604844]\n",
      " [0.93210535]]\n"
     ]
    }
   ],
   "source": [
    "print('MSE: ' + str(loss_function_mse(output[-1],y)))\n",
    "print('Estimation: ' + str(output[-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traversing in the model\n",
    "\n",
    "def network_propagation (X, y, network, lr=0.01):\n",
    "    \n",
    "    # forward propagation\n",
    "    output = [X] # put input as the fist layer output\n",
    "    for layer in range(len(network)):\n",
    "        network[layer].forward(output[-1])\n",
    "        z = network[layer].output\n",
    "        a = activate_sigmoid(z)\n",
    "        output.append(a)\n",
    "    \n",
    "    # backforward propagation for output and errors\n",
    "    delta = []\n",
    "    back = list(range(len(output)-1))\n",
    "    back.reverse()\n",
    "\n",
    "    for layer in back:\n",
    "        # get one of the output of next layer\n",
    "        next_a = output[layer+1]\n",
    "        if layer == back[0]:\n",
    "            x = (next_a - y) * der_sigmoid(next_a)\n",
    "            delta.append(x)\n",
    "        else:\n",
    "            x = delta[-1] @ previous_W * der_sigmoid(next_a)\n",
    "            delta.append(x)\n",
    "        # store tmp W for next layer previous compute\n",
    "        previous_W = network[layer].weights.transpose()\n",
    "\n",
    "        # gradicent descent for update weights\n",
    "        network[layer].biases = network[layer].biases - np.mean(delta[-1], axis = 0, keepdims = True) * lr\n",
    "        network[layer].weights = network[layer].weights - output[layer].T @ delta[-1] * lr\n",
    "#       print(\"===> network {} layer, weights => {} bias =>{} updated\".format(i, network[i].weights, network[i].biases))\n",
    "    \n",
    "    return output[-1], delta\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, network, lr=0.01, epoches=100):\n",
    "    errors = []\n",
    "    predictions = []\n",
    "    delta_history = []\n",
    "    for i in range(epoches):\n",
    "        prediction, delta = network_propagation(X, y, network, lr=lr)\n",
    "        delta_history.append(delta)\n",
    "        predictions.append(prediction)\n",
    "        error = loss_function_mse(prediction, y)\n",
    "        print(\"===>> epoch: {},  error: {:.4f}, accuracy {:.4f} %\".format(i , error, (1- error)*100))\n",
    "        errors.append(error)\n",
    "        \n",
    "    return errors, predictions, delta_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.6596134 ],\n",
       "        [0.65570321],\n",
       "        [0.65156763],\n",
       "        [0.64859885],\n",
       "        [0.68290276],\n",
       "        [0.68239106],\n",
       "        [0.67637536],\n",
       "        [0.66023   ]]), [array([[ 0.14809875],\n",
       "         [ 0.14802927],\n",
       "         [ 0.14792361],\n",
       "         [ 0.1478276 ],\n",
       "         [-0.06866632],\n",
       "         [-0.0688365 ],\n",
       "         [-0.07083876],\n",
       "         [ 0.14810698]]), array([[ 0.0169465 ,  0.05309586],\n",
       "         [ 0.01628604,  0.05282026],\n",
       "         [ 0.01576037,  0.05225811],\n",
       "         [ 0.01538843,  0.05180879],\n",
       "         [-0.00839023, -0.02631156],\n",
       "         [-0.00839655, -0.02635113],\n",
       "         [-0.00857285, -0.02663253],\n",
       "         [ 0.01770895,  0.05222605]]), array([[-1.42820318e-02, -3.26189535e-03,  6.64364117e-04,\n",
       "          -9.77917321e-03],\n",
       "         [-1.29166372e-02, -1.31091202e-03,  2.88898915e-04,\n",
       "          -9.43329509e-03],\n",
       "         [-1.03287408e-02, -5.07316294e-04,  1.22776851e-04,\n",
       "          -8.69563028e-03],\n",
       "         [-7.51013567e-03, -1.94183235e-04,  5.18855077e-05,\n",
       "          -7.73043558e-03],\n",
       "         [ 6.35338942e-04,  9.26339335e-06, -1.09805191e-07,\n",
       "           1.67515081e-03],\n",
       "         [ 2.25090308e-03,  3.28666080e-04, -1.45399240e-05,\n",
       "           3.06421844e-03],\n",
       "         [ 4.57852407e-03,  1.17955809e-03, -1.08448072e-04,\n",
       "           4.13669814e-03],\n",
       "         [-1.36516488e-02, -7.33898663e-03,  1.43657694e-03,\n",
       "          -9.54820205e-03]])])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_propagation(X_inputs, y, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define data set and test set\n",
    "X = np.array([[1,2],[1,3],[1,4],[1,5],[5,2],[3,1],[2,1],[1,1]])\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 0]])\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 0]]).reshape(y.shape[1],1)\n",
    "\n",
    "\n",
    "test_set = [[6,1],[-1, -2],[-2, 3],[3, 2],[3,2], [7,1], [-2,-2],[1,1]]\n",
    "y_test_set = np.array([[1, 1, 0, 1, 1, 1, 0, 0]])\n",
    "y_test_set = np.array([[1, 1, 0, 1, 1, 1, 0, 0]]).reshape(y_test_set.shape[1],1)\n",
    "\n",
    "\n",
    "# Build network with hyperparameters\n",
    "learning_rate = 0.5\n",
    "epoches = 500\n",
    "networks = [2,4,2,1] # train with three layers with neuron 4,2,1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>> epoch: 0,  error: 0.2266, accuracy 77.3381 %\n",
      "===>> epoch: 1,  error: 0.2259, accuracy 77.4103 %\n",
      "===>> epoch: 2,  error: 0.2251, accuracy 77.4889 %\n",
      "===>> epoch: 3,  error: 0.2242, accuracy 77.5760 %\n",
      "===>> epoch: 4,  error: 0.2233, accuracy 77.6736 %\n",
      "===>> epoch: 5,  error: 0.2222, accuracy 77.7844 %\n",
      "===>> epoch: 6,  error: 0.2209, accuracy 77.9116 %\n",
      "===>> epoch: 7,  error: 0.2194, accuracy 78.0594 %\n",
      "===>> epoch: 8,  error: 0.2177, accuracy 78.2326 %\n",
      "===>> epoch: 9,  error: 0.2156, accuracy 78.4360 %\n",
      "===>> epoch: 10,  error: 0.2133, accuracy 78.6738 %\n",
      "===>> epoch: 11,  error: 0.2105, accuracy 78.9467 %\n",
      "===>> epoch: 12,  error: 0.2075, accuracy 79.2500 %\n",
      "===>> epoch: 13,  error: 0.2043, accuracy 79.5734 %\n",
      "===>> epoch: 14,  error: 0.2010, accuracy 79.9043 %\n",
      "===>> epoch: 15,  error: 0.1977, accuracy 80.2324 %\n",
      "===>> epoch: 16,  error: 0.1945, accuracy 80.5519 %\n",
      "===>> epoch: 17,  error: 0.1914, accuracy 80.8611 %\n",
      "===>> epoch: 18,  error: 0.1884, accuracy 81.1601 %\n",
      "===>> epoch: 19,  error: 0.1855, accuracy 81.4500 %\n",
      "===>> epoch: 20,  error: 0.1827, accuracy 81.7321 %\n",
      "===>> epoch: 21,  error: 0.1799, accuracy 82.0074 %\n",
      "===>> epoch: 22,  error: 0.1772, accuracy 82.2770 %\n",
      "===>> epoch: 23,  error: 0.1746, accuracy 82.5415 %\n",
      "===>> epoch: 24,  error: 0.1720, accuracy 82.8016 %\n",
      "===>> epoch: 25,  error: 0.1694, accuracy 83.0577 %\n",
      "===>> epoch: 26,  error: 0.1669, accuracy 83.3100 %\n",
      "===>> epoch: 27,  error: 0.1644, accuracy 83.5588 %\n",
      "===>> epoch: 28,  error: 0.1620, accuracy 83.8041 %\n",
      "===>> epoch: 29,  error: 0.1595, accuracy 84.0461 %\n",
      "===>> epoch: 30,  error: 0.1572, accuracy 84.2848 %\n",
      "===>> epoch: 31,  error: 0.1548, accuracy 84.5202 %\n",
      "===>> epoch: 32,  error: 0.1525, accuracy 84.7522 %\n",
      "===>> epoch: 33,  error: 0.1502, accuracy 84.9809 %\n",
      "===>> epoch: 34,  error: 0.1479, accuracy 85.2061 %\n",
      "===>> epoch: 35,  error: 0.1457, accuracy 85.4279 %\n",
      "===>> epoch: 36,  error: 0.1435, accuracy 85.6462 %\n",
      "===>> epoch: 37,  error: 0.1414, accuracy 85.8610 %\n",
      "===>> epoch: 38,  error: 0.1393, accuracy 86.0723 %\n",
      "===>> epoch: 39,  error: 0.1372, accuracy 86.2800 %\n",
      "===>> epoch: 40,  error: 0.1352, accuracy 86.4842 %\n",
      "===>> epoch: 41,  error: 0.1332, accuracy 86.6848 %\n",
      "===>> epoch: 42,  error: 0.1312, accuracy 86.8819 %\n",
      "===>> epoch: 43,  error: 0.1292, accuracy 87.0755 %\n",
      "===>> epoch: 44,  error: 0.1273, accuracy 87.2656 %\n",
      "===>> epoch: 45,  error: 0.1255, accuracy 87.4523 %\n",
      "===>> epoch: 46,  error: 0.1236, accuracy 87.6355 %\n",
      "===>> epoch: 47,  error: 0.1218, accuracy 87.8155 %\n",
      "===>> epoch: 48,  error: 0.1201, accuracy 87.9921 %\n",
      "===>> epoch: 49,  error: 0.1183, accuracy 88.1654 %\n",
      "===>> epoch: 50,  error: 0.1166, accuracy 88.3356 %\n",
      "===>> epoch: 51,  error: 0.1150, accuracy 88.5027 %\n",
      "===>> epoch: 52,  error: 0.1133, accuracy 88.6667 %\n",
      "===>> epoch: 53,  error: 0.1117, accuracy 88.8277 %\n",
      "===>> epoch: 54,  error: 0.1101, accuracy 88.9859 %\n",
      "===>> epoch: 55,  error: 0.1086, accuracy 89.1412 %\n",
      "===>> epoch: 56,  error: 0.1071, accuracy 89.2938 %\n",
      "===>> epoch: 57,  error: 0.1056, accuracy 89.4437 %\n",
      "===>> epoch: 58,  error: 0.1041, accuracy 89.5911 %\n",
      "===>> epoch: 59,  error: 0.1026, accuracy 89.7359 %\n",
      "===>> epoch: 60,  error: 0.1012, accuracy 89.8783 %\n",
      "===>> epoch: 61,  error: 0.0998, accuracy 90.0183 %\n",
      "===>> epoch: 62,  error: 0.0984, accuracy 90.1560 %\n",
      "===>> epoch: 63,  error: 0.0971, accuracy 90.2916 %\n",
      "===>> epoch: 64,  error: 0.0958, accuracy 90.4250 %\n",
      "===>> epoch: 65,  error: 0.0944, accuracy 90.5563 %\n",
      "===>> epoch: 66,  error: 0.0931, accuracy 90.6857 %\n",
      "===>> epoch: 67,  error: 0.0919, accuracy 90.8131 %\n",
      "===>> epoch: 68,  error: 0.0906, accuracy 90.9387 %\n",
      "===>> epoch: 69,  error: 0.0894, accuracy 91.0624 %\n",
      "===>> epoch: 70,  error: 0.0882, accuracy 91.1845 %\n",
      "===>> epoch: 71,  error: 0.0870, accuracy 91.3049 %\n",
      "===>> epoch: 72,  error: 0.0858, accuracy 91.4236 %\n",
      "===>> epoch: 73,  error: 0.0846, accuracy 91.5408 %\n",
      "===>> epoch: 74,  error: 0.0834, accuracy 91.6565 %\n",
      "===>> epoch: 75,  error: 0.0823, accuracy 91.7708 %\n",
      "===>> epoch: 76,  error: 0.0812, accuracy 91.8836 %\n",
      "===>> epoch: 77,  error: 0.0800, accuracy 91.9951 %\n",
      "===>> epoch: 78,  error: 0.0789, accuracy 92.1052 %\n",
      "===>> epoch: 79,  error: 0.0779, accuracy 92.2141 %\n",
      "===>> epoch: 80,  error: 0.0768, accuracy 92.3218 %\n",
      "===>> epoch: 81,  error: 0.0757, accuracy 92.4283 %\n",
      "===>> epoch: 82,  error: 0.0747, accuracy 92.5336 %\n",
      "===>> epoch: 83,  error: 0.0736, accuracy 92.6377 %\n",
      "===>> epoch: 84,  error: 0.0726, accuracy 92.7408 %\n",
      "===>> epoch: 85,  error: 0.0716, accuracy 92.8429 %\n",
      "===>> epoch: 86,  error: 0.0706, accuracy 92.9439 %\n",
      "===>> epoch: 87,  error: 0.0696, accuracy 93.0439 %\n",
      "===>> epoch: 88,  error: 0.0686, accuracy 93.1430 %\n",
      "===>> epoch: 89,  error: 0.0676, accuracy 93.2411 %\n",
      "===>> epoch: 90,  error: 0.0666, accuracy 93.3383 %\n",
      "===>> epoch: 91,  error: 0.0657, accuracy 93.4347 %\n",
      "===>> epoch: 92,  error: 0.0647, accuracy 93.5302 %\n",
      "===>> epoch: 93,  error: 0.0638, accuracy 93.6248 %\n",
      "===>> epoch: 94,  error: 0.0628, accuracy 93.7187 %\n",
      "===>> epoch: 95,  error: 0.0619, accuracy 93.8117 %\n",
      "===>> epoch: 96,  error: 0.0610, accuracy 93.9040 %\n",
      "===>> epoch: 97,  error: 0.0600, accuracy 93.9956 %\n",
      "===>> epoch: 98,  error: 0.0591, accuracy 94.0865 %\n",
      "===>> epoch: 99,  error: 0.0582, accuracy 94.1767 %\n",
      "===>> epoch: 100,  error: 0.0573, accuracy 94.2662 %\n",
      "===>> epoch: 101,  error: 0.0564, accuracy 94.3550 %\n",
      "===>> epoch: 102,  error: 0.0556, accuracy 94.4433 %\n",
      "===>> epoch: 103,  error: 0.0547, accuracy 94.5309 %\n",
      "===>> epoch: 104,  error: 0.0538, accuracy 94.6180 %\n",
      "===>> epoch: 105,  error: 0.0530, accuracy 94.7045 %\n",
      "===>> epoch: 106,  error: 0.0521, accuracy 94.7904 %\n",
      "===>> epoch: 107,  error: 0.0512, accuracy 94.8758 %\n",
      "===>> epoch: 108,  error: 0.0504, accuracy 94.9606 %\n",
      "===>> epoch: 109,  error: 0.0496, accuracy 95.0450 %\n",
      "===>> epoch: 110,  error: 0.0487, accuracy 95.1288 %\n",
      "===>> epoch: 111,  error: 0.0479, accuracy 95.2122 %\n",
      "===>> epoch: 112,  error: 0.0471, accuracy 95.2950 %\n",
      "===>> epoch: 113,  error: 0.0462, accuracy 95.3773 %\n",
      "===>> epoch: 114,  error: 0.0454, accuracy 95.4592 %\n",
      "===>> epoch: 115,  error: 0.0446, accuracy 95.5405 %\n",
      "===>> epoch: 116,  error: 0.0438, accuracy 95.6214 %\n",
      "===>> epoch: 117,  error: 0.0430, accuracy 95.7017 %\n",
      "===>> epoch: 118,  error: 0.0422, accuracy 95.7815 %\n",
      "===>> epoch: 119,  error: 0.0414, accuracy 95.8607 %\n",
      "===>> epoch: 120,  error: 0.0406, accuracy 95.9393 %\n",
      "===>> epoch: 121,  error: 0.0398, accuracy 96.0173 %\n",
      "===>> epoch: 122,  error: 0.0391, accuracy 96.0946 %\n",
      "===>> epoch: 123,  error: 0.0383, accuracy 96.1713 %\n",
      "===>> epoch: 124,  error: 0.0375, accuracy 96.2472 %\n",
      "===>> epoch: 125,  error: 0.0368, accuracy 96.3224 %\n",
      "===>> epoch: 126,  error: 0.0360, accuracy 96.3967 %\n",
      "===>> epoch: 127,  error: 0.0353, accuracy 96.4702 %\n",
      "===>> epoch: 128,  error: 0.0346, accuracy 96.5428 %\n",
      "===>> epoch: 129,  error: 0.0339, accuracy 96.6144 %\n",
      "===>> epoch: 130,  error: 0.0331, accuracy 96.6850 %\n",
      "===>> epoch: 131,  error: 0.0325, accuracy 96.7546 %\n",
      "===>> epoch: 132,  error: 0.0318, accuracy 96.8230 %\n",
      "===>> epoch: 133,  error: 0.0311, accuracy 96.8903 %\n",
      "===>> epoch: 134,  error: 0.0304, accuracy 96.9564 %\n",
      "===>> epoch: 135,  error: 0.0298, accuracy 97.0213 %\n",
      "===>> epoch: 136,  error: 0.0292, accuracy 97.0849 %\n",
      "===>> epoch: 137,  error: 0.0285, accuracy 97.1472 %\n",
      "===>> epoch: 138,  error: 0.0279, accuracy 97.2082 %\n",
      "===>> epoch: 139,  error: 0.0273, accuracy 97.2679 %\n",
      "===>> epoch: 140,  error: 0.0267, accuracy 97.3262 %\n",
      "===>> epoch: 141,  error: 0.0262, accuracy 97.3831 %\n",
      "===>> epoch: 142,  error: 0.0256, accuracy 97.4386 %\n",
      "===>> epoch: 143,  error: 0.0251, accuracy 97.4928 %\n",
      "===>> epoch: 144,  error: 0.0245, accuracy 97.5456 %\n",
      "===>> epoch: 145,  error: 0.0240, accuracy 97.5970 %\n",
      "===>> epoch: 146,  error: 0.0235, accuracy 97.6471 %\n",
      "===>> epoch: 147,  error: 0.0230, accuracy 97.6958 %\n",
      "===>> epoch: 148,  error: 0.0226, accuracy 97.7432 %\n",
      "===>> epoch: 149,  error: 0.0221, accuracy 97.7893 %\n",
      "===>> epoch: 150,  error: 0.0217, accuracy 97.8341 %\n",
      "===>> epoch: 151,  error: 0.0212, accuracy 97.8777 %\n",
      "===>> epoch: 152,  error: 0.0208, accuracy 97.9200 %\n",
      "===>> epoch: 153,  error: 0.0204, accuracy 97.9611 %\n",
      "===>> epoch: 154,  error: 0.0200, accuracy 98.0010 %\n",
      "===>> epoch: 155,  error: 0.0196, accuracy 98.0399 %\n",
      "===>> epoch: 156,  error: 0.0192, accuracy 98.0775 %\n",
      "===>> epoch: 157,  error: 0.0189, accuracy 98.1142 %\n",
      "===>> epoch: 158,  error: 0.0185, accuracy 98.1497 %\n",
      "===>> epoch: 159,  error: 0.0182, accuracy 98.1843 %\n",
      "===>> epoch: 160,  error: 0.0178, accuracy 98.2179 %\n",
      "===>> epoch: 161,  error: 0.0175, accuracy 98.2505 %\n",
      "===>> epoch: 162,  error: 0.0172, accuracy 98.2822 %\n",
      "===>> epoch: 163,  error: 0.0169, accuracy 98.3130 %\n",
      "===>> epoch: 164,  error: 0.0166, accuracy 98.3429 %\n",
      "===>> epoch: 165,  error: 0.0163, accuracy 98.3720 %\n",
      "===>> epoch: 166,  error: 0.0160, accuracy 98.4003 %\n",
      "===>> epoch: 167,  error: 0.0157, accuracy 98.4278 %\n",
      "===>> epoch: 168,  error: 0.0155, accuracy 98.4546 %\n",
      "===>> epoch: 169,  error: 0.0152, accuracy 98.4806 %\n",
      "===>> epoch: 170,  error: 0.0149, accuracy 98.5059 %\n",
      "===>> epoch: 171,  error: 0.0147, accuracy 98.5306 %\n",
      "===>> epoch: 172,  error: 0.0145, accuracy 98.5546 %\n",
      "===>> epoch: 173,  error: 0.0142, accuracy 98.5779 %\n",
      "===>> epoch: 174,  error: 0.0140, accuracy 98.6007 %\n",
      "===>> epoch: 175,  error: 0.0138, accuracy 98.6228 %\n",
      "===>> epoch: 176,  error: 0.0136, accuracy 98.6444 %\n",
      "===>> epoch: 177,  error: 0.0133, accuracy 98.6655 %\n",
      "===>> epoch: 178,  error: 0.0131, accuracy 98.6860 %\n",
      "===>> epoch: 179,  error: 0.0129, accuracy 98.7059 %\n",
      "===>> epoch: 180,  error: 0.0127, accuracy 98.7254 %\n",
      "===>> epoch: 181,  error: 0.0126, accuracy 98.7444 %\n",
      "===>> epoch: 182,  error: 0.0124, accuracy 98.7630 %\n",
      "===>> epoch: 183,  error: 0.0122, accuracy 98.7811 %\n",
      "===>> epoch: 184,  error: 0.0120, accuracy 98.7987 %\n",
      "===>> epoch: 185,  error: 0.0118, accuracy 98.8159 %\n",
      "===>> epoch: 186,  error: 0.0117, accuracy 98.8328 %\n",
      "===>> epoch: 187,  error: 0.0115, accuracy 98.8492 %\n",
      "===>> epoch: 188,  error: 0.0113, accuracy 98.8652 %\n",
      "===>> epoch: 189,  error: 0.0112, accuracy 98.8809 %\n",
      "===>> epoch: 190,  error: 0.0110, accuracy 98.8962 %\n",
      "===>> epoch: 191,  error: 0.0109, accuracy 98.9112 %\n",
      "===>> epoch: 192,  error: 0.0107, accuracy 98.9258 %\n",
      "===>> epoch: 193,  error: 0.0106, accuracy 98.9401 %\n",
      "===>> epoch: 194,  error: 0.0105, accuracy 98.9541 %\n",
      "===>> epoch: 195,  error: 0.0103, accuracy 98.9678 %\n",
      "===>> epoch: 196,  error: 0.0102, accuracy 98.9811 %\n",
      "===>> epoch: 197,  error: 0.0101, accuracy 98.9942 %\n",
      "===>> epoch: 198,  error: 0.0099, accuracy 99.0070 %\n",
      "===>> epoch: 199,  error: 0.0098, accuracy 99.0196 %\n",
      "===>> epoch: 200,  error: 0.0097, accuracy 99.0318 %\n",
      "===>> epoch: 201,  error: 0.0096, accuracy 99.0438 %\n",
      "===>> epoch: 202,  error: 0.0094, accuracy 99.0556 %\n",
      "===>> epoch: 203,  error: 0.0093, accuracy 99.0671 %\n",
      "===>> epoch: 204,  error: 0.0092, accuracy 99.0784 %\n",
      "===>> epoch: 205,  error: 0.0091, accuracy 99.0895 %\n",
      "===>> epoch: 206,  error: 0.0090, accuracy 99.1003 %\n",
      "===>> epoch: 207,  error: 0.0089, accuracy 99.1109 %\n",
      "===>> epoch: 208,  error: 0.0088, accuracy 99.1213 %\n",
      "===>> epoch: 209,  error: 0.0087, accuracy 99.1315 %\n",
      "===>> epoch: 210,  error: 0.0086, accuracy 99.1415 %\n",
      "===>> epoch: 211,  error: 0.0085, accuracy 99.1514 %\n",
      "===>> epoch: 212,  error: 0.0084, accuracy 99.1610 %\n",
      "===>> epoch: 213,  error: 0.0083, accuracy 99.1704 %\n",
      "===>> epoch: 214,  error: 0.0082, accuracy 99.1797 %\n",
      "===>> epoch: 215,  error: 0.0081, accuracy 99.1888 %\n",
      "===>> epoch: 216,  error: 0.0080, accuracy 99.1977 %\n",
      "===>> epoch: 217,  error: 0.0079, accuracy 99.2064 %\n",
      "===>> epoch: 218,  error: 0.0078, accuracy 99.2150 %\n",
      "===>> epoch: 219,  error: 0.0078, accuracy 99.2235 %\n",
      "===>> epoch: 220,  error: 0.0077, accuracy 99.2317 %\n",
      "===>> epoch: 221,  error: 0.0076, accuracy 99.2399 %\n",
      "===>> epoch: 222,  error: 0.0075, accuracy 99.2478 %\n",
      "===>> epoch: 223,  error: 0.0074, accuracy 99.2557 %\n",
      "===>> epoch: 224,  error: 0.0074, accuracy 99.2634 %\n",
      "===>> epoch: 225,  error: 0.0073, accuracy 99.2710 %\n",
      "===>> epoch: 226,  error: 0.0072, accuracy 99.2784 %\n",
      "===>> epoch: 227,  error: 0.0071, accuracy 99.2857 %\n",
      "===>> epoch: 228,  error: 0.0071, accuracy 99.2929 %\n",
      "===>> epoch: 229,  error: 0.0070, accuracy 99.3000 %\n",
      "===>> epoch: 230,  error: 0.0069, accuracy 99.3069 %\n",
      "===>> epoch: 231,  error: 0.0069, accuracy 99.3137 %\n",
      "===>> epoch: 232,  error: 0.0068, accuracy 99.3204 %\n",
      "===>> epoch: 233,  error: 0.0067, accuracy 99.3270 %\n",
      "===>> epoch: 234,  error: 0.0067, accuracy 99.3335 %\n",
      "===>> epoch: 235,  error: 0.0066, accuracy 99.3399 %\n",
      "===>> epoch: 236,  error: 0.0065, accuracy 99.3462 %\n",
      "===>> epoch: 237,  error: 0.0065, accuracy 99.3524 %\n",
      "===>> epoch: 238,  error: 0.0064, accuracy 99.3584 %\n",
      "===>> epoch: 239,  error: 0.0064, accuracy 99.3644 %\n",
      "===>> epoch: 240,  error: 0.0063, accuracy 99.3703 %\n",
      "===>> epoch: 241,  error: 0.0062, accuracy 99.3761 %\n",
      "===>> epoch: 242,  error: 0.0062, accuracy 99.3818 %\n",
      "===>> epoch: 243,  error: 0.0061, accuracy 99.3874 %\n",
      "===>> epoch: 244,  error: 0.0061, accuracy 99.3930 %\n",
      "===>> epoch: 245,  error: 0.0060, accuracy 99.3984 %\n",
      "===>> epoch: 246,  error: 0.0060, accuracy 99.4038 %\n",
      "===>> epoch: 247,  error: 0.0059, accuracy 99.4090 %\n",
      "===>> epoch: 248,  error: 0.0059, accuracy 99.4142 %\n",
      "===>> epoch: 249,  error: 0.0058, accuracy 99.4193 %\n",
      "===>> epoch: 250,  error: 0.0058, accuracy 99.4244 %\n",
      "===>> epoch: 251,  error: 0.0057, accuracy 99.4294 %\n",
      "===>> epoch: 252,  error: 0.0057, accuracy 99.4343 %\n",
      "===>> epoch: 253,  error: 0.0056, accuracy 99.4391 %\n",
      "===>> epoch: 254,  error: 0.0056, accuracy 99.4438 %\n",
      "===>> epoch: 255,  error: 0.0055, accuracy 99.4485 %\n",
      "===>> epoch: 256,  error: 0.0055, accuracy 99.4531 %\n",
      "===>> epoch: 257,  error: 0.0054, accuracy 99.4577 %\n",
      "===>> epoch: 258,  error: 0.0054, accuracy 99.4622 %\n",
      "===>> epoch: 259,  error: 0.0053, accuracy 99.4666 %\n",
      "===>> epoch: 260,  error: 0.0053, accuracy 99.4709 %\n",
      "===>> epoch: 261,  error: 0.0052, accuracy 99.4752 %\n",
      "===>> epoch: 262,  error: 0.0052, accuracy 99.4795 %\n",
      "===>> epoch: 263,  error: 0.0052, accuracy 99.4837 %\n",
      "===>> epoch: 264,  error: 0.0051, accuracy 99.4878 %\n",
      "===>> epoch: 265,  error: 0.0051, accuracy 99.4918 %\n",
      "===>> epoch: 266,  error: 0.0050, accuracy 99.4959 %\n",
      "===>> epoch: 267,  error: 0.0050, accuracy 99.4998 %\n",
      "===>> epoch: 268,  error: 0.0050, accuracy 99.5037 %\n",
      "===>> epoch: 269,  error: 0.0049, accuracy 99.5076 %\n",
      "===>> epoch: 270,  error: 0.0049, accuracy 99.5114 %\n",
      "===>> epoch: 271,  error: 0.0048, accuracy 99.5151 %\n",
      "===>> epoch: 272,  error: 0.0048, accuracy 99.5188 %\n",
      "===>> epoch: 273,  error: 0.0048, accuracy 99.5224 %\n",
      "===>> epoch: 274,  error: 0.0047, accuracy 99.5260 %\n",
      "===>> epoch: 275,  error: 0.0047, accuracy 99.5296 %\n",
      "===>> epoch: 276,  error: 0.0047, accuracy 99.5331 %\n",
      "===>> epoch: 277,  error: 0.0046, accuracy 99.5366 %\n",
      "===>> epoch: 278,  error: 0.0046, accuracy 99.5400 %\n",
      "===>> epoch: 279,  error: 0.0046, accuracy 99.5434 %\n",
      "===>> epoch: 280,  error: 0.0045, accuracy 99.5467 %\n",
      "===>> epoch: 281,  error: 0.0045, accuracy 99.5500 %\n",
      "===>> epoch: 282,  error: 0.0045, accuracy 99.5532 %\n",
      "===>> epoch: 283,  error: 0.0044, accuracy 99.5565 %\n",
      "===>> epoch: 284,  error: 0.0044, accuracy 99.5596 %\n",
      "===>> epoch: 285,  error: 0.0044, accuracy 99.5627 %\n",
      "===>> epoch: 286,  error: 0.0043, accuracy 99.5658 %\n",
      "===>> epoch: 287,  error: 0.0043, accuracy 99.5689 %\n",
      "===>> epoch: 288,  error: 0.0043, accuracy 99.5719 %\n",
      "===>> epoch: 289,  error: 0.0043, accuracy 99.5749 %\n",
      "===>> epoch: 290,  error: 0.0042, accuracy 99.5778 %\n",
      "===>> epoch: 291,  error: 0.0042, accuracy 99.5807 %\n",
      "===>> epoch: 292,  error: 0.0042, accuracy 99.5836 %\n",
      "===>> epoch: 293,  error: 0.0041, accuracy 99.5865 %\n",
      "===>> epoch: 294,  error: 0.0041, accuracy 99.5893 %\n",
      "===>> epoch: 295,  error: 0.0041, accuracy 99.5920 %\n",
      "===>> epoch: 296,  error: 0.0041, accuracy 99.5948 %\n",
      "===>> epoch: 297,  error: 0.0040, accuracy 99.5975 %\n",
      "===>> epoch: 298,  error: 0.0040, accuracy 99.6002 %\n",
      "===>> epoch: 299,  error: 0.0040, accuracy 99.6028 %\n",
      "===>> epoch: 300,  error: 0.0039, accuracy 99.6054 %\n",
      "===>> epoch: 301,  error: 0.0039, accuracy 99.6080 %\n",
      "===>> epoch: 302,  error: 0.0039, accuracy 99.6106 %\n",
      "===>> epoch: 303,  error: 0.0039, accuracy 99.6131 %\n",
      "===>> epoch: 304,  error: 0.0038, accuracy 99.6156 %\n",
      "===>> epoch: 305,  error: 0.0038, accuracy 99.6181 %\n",
      "===>> epoch: 306,  error: 0.0038, accuracy 99.6205 %\n",
      "===>> epoch: 307,  error: 0.0038, accuracy 99.6229 %\n",
      "===>> epoch: 308,  error: 0.0037, accuracy 99.6253 %\n",
      "===>> epoch: 309,  error: 0.0037, accuracy 99.6277 %\n",
      "===>> epoch: 310,  error: 0.0037, accuracy 99.6300 %\n",
      "===>> epoch: 311,  error: 0.0037, accuracy 99.6323 %\n",
      "===>> epoch: 312,  error: 0.0037, accuracy 99.6346 %\n",
      "===>> epoch: 313,  error: 0.0036, accuracy 99.6369 %\n",
      "===>> epoch: 314,  error: 0.0036, accuracy 99.6391 %\n",
      "===>> epoch: 315,  error: 0.0036, accuracy 99.6413 %\n",
      "===>> epoch: 316,  error: 0.0036, accuracy 99.6435 %\n",
      "===>> epoch: 317,  error: 0.0035, accuracy 99.6457 %\n",
      "===>> epoch: 318,  error: 0.0035, accuracy 99.6478 %\n",
      "===>> epoch: 319,  error: 0.0035, accuracy 99.6499 %\n",
      "===>> epoch: 320,  error: 0.0035, accuracy 99.6520 %\n",
      "===>> epoch: 321,  error: 0.0035, accuracy 99.6541 %\n",
      "===>> epoch: 322,  error: 0.0034, accuracy 99.6562 %\n",
      "===>> epoch: 323,  error: 0.0034, accuracy 99.6582 %\n",
      "===>> epoch: 324,  error: 0.0034, accuracy 99.6602 %\n",
      "===>> epoch: 325,  error: 0.0034, accuracy 99.6622 %\n",
      "===>> epoch: 326,  error: 0.0034, accuracy 99.6642 %\n",
      "===>> epoch: 327,  error: 0.0033, accuracy 99.6661 %\n",
      "===>> epoch: 328,  error: 0.0033, accuracy 99.6680 %\n",
      "===>> epoch: 329,  error: 0.0033, accuracy 99.6700 %\n",
      "===>> epoch: 330,  error: 0.0033, accuracy 99.6719 %\n",
      "===>> epoch: 331,  error: 0.0033, accuracy 99.6737 %\n",
      "===>> epoch: 332,  error: 0.0032, accuracy 99.6756 %\n",
      "===>> epoch: 333,  error: 0.0032, accuracy 99.6774 %\n",
      "===>> epoch: 334,  error: 0.0032, accuracy 99.6792 %\n",
      "===>> epoch: 335,  error: 0.0032, accuracy 99.6810 %\n",
      "===>> epoch: 336,  error: 0.0032, accuracy 99.6828 %\n",
      "===>> epoch: 337,  error: 0.0032, accuracy 99.6846 %\n",
      "===>> epoch: 338,  error: 0.0031, accuracy 99.6863 %\n",
      "===>> epoch: 339,  error: 0.0031, accuracy 99.6881 %\n",
      "===>> epoch: 340,  error: 0.0031, accuracy 99.6898 %\n",
      "===>> epoch: 341,  error: 0.0031, accuracy 99.6915 %\n",
      "===>> epoch: 342,  error: 0.0031, accuracy 99.6931 %\n",
      "===>> epoch: 343,  error: 0.0031, accuracy 99.6948 %\n",
      "===>> epoch: 344,  error: 0.0030, accuracy 99.6965 %\n",
      "===>> epoch: 345,  error: 0.0030, accuracy 99.6981 %\n",
      "===>> epoch: 346,  error: 0.0030, accuracy 99.6997 %\n",
      "===>> epoch: 347,  error: 0.0030, accuracy 99.7013 %\n",
      "===>> epoch: 348,  error: 0.0030, accuracy 99.7029 %\n",
      "===>> epoch: 349,  error: 0.0030, accuracy 99.7045 %\n",
      "===>> epoch: 350,  error: 0.0029, accuracy 99.7060 %\n",
      "===>> epoch: 351,  error: 0.0029, accuracy 99.7076 %\n",
      "===>> epoch: 352,  error: 0.0029, accuracy 99.7091 %\n",
      "===>> epoch: 353,  error: 0.0029, accuracy 99.7106 %\n",
      "===>> epoch: 354,  error: 0.0029, accuracy 99.7121 %\n",
      "===>> epoch: 355,  error: 0.0029, accuracy 99.7136 %\n",
      "===>> epoch: 356,  error: 0.0028, accuracy 99.7151 %\n",
      "===>> epoch: 357,  error: 0.0028, accuracy 99.7165 %\n",
      "===>> epoch: 358,  error: 0.0028, accuracy 99.7180 %\n",
      "===>> epoch: 359,  error: 0.0028, accuracy 99.7194 %\n",
      "===>> epoch: 360,  error: 0.0028, accuracy 99.7208 %\n",
      "===>> epoch: 361,  error: 0.0028, accuracy 99.7223 %\n",
      "===>> epoch: 362,  error: 0.0028, accuracy 99.7237 %\n",
      "===>> epoch: 363,  error: 0.0027, accuracy 99.7250 %\n",
      "===>> epoch: 364,  error: 0.0027, accuracy 99.7264 %\n",
      "===>> epoch: 365,  error: 0.0027, accuracy 99.7278 %\n",
      "===>> epoch: 366,  error: 0.0027, accuracy 99.7291 %\n",
      "===>> epoch: 367,  error: 0.0027, accuracy 99.7305 %\n",
      "===>> epoch: 368,  error: 0.0027, accuracy 99.7318 %\n",
      "===>> epoch: 369,  error: 0.0027, accuracy 99.7331 %\n",
      "===>> epoch: 370,  error: 0.0027, accuracy 99.7344 %\n",
      "===>> epoch: 371,  error: 0.0026, accuracy 99.7357 %\n",
      "===>> epoch: 372,  error: 0.0026, accuracy 99.7370 %\n",
      "===>> epoch: 373,  error: 0.0026, accuracy 99.7382 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>> epoch: 374,  error: 0.0026, accuracy 99.7395 %\n",
      "===>> epoch: 375,  error: 0.0026, accuracy 99.7407 %\n",
      "===>> epoch: 376,  error: 0.0026, accuracy 99.7420 %\n",
      "===>> epoch: 377,  error: 0.0026, accuracy 99.7432 %\n",
      "===>> epoch: 378,  error: 0.0026, accuracy 99.7444 %\n",
      "===>> epoch: 379,  error: 0.0025, accuracy 99.7456 %\n",
      "===>> epoch: 380,  error: 0.0025, accuracy 99.7468 %\n",
      "===>> epoch: 381,  error: 0.0025, accuracy 99.7480 %\n",
      "===>> epoch: 382,  error: 0.0025, accuracy 99.7492 %\n",
      "===>> epoch: 383,  error: 0.0025, accuracy 99.7503 %\n",
      "===>> epoch: 384,  error: 0.0025, accuracy 99.7515 %\n",
      "===>> epoch: 385,  error: 0.0025, accuracy 99.7526 %\n",
      "===>> epoch: 386,  error: 0.0025, accuracy 99.7538 %\n",
      "===>> epoch: 387,  error: 0.0025, accuracy 99.7549 %\n",
      "===>> epoch: 388,  error: 0.0024, accuracy 99.7560 %\n",
      "===>> epoch: 389,  error: 0.0024, accuracy 99.7571 %\n",
      "===>> epoch: 390,  error: 0.0024, accuracy 99.7582 %\n",
      "===>> epoch: 391,  error: 0.0024, accuracy 99.7593 %\n",
      "===>> epoch: 392,  error: 0.0024, accuracy 99.7604 %\n",
      "===>> epoch: 393,  error: 0.0024, accuracy 99.7615 %\n",
      "===>> epoch: 394,  error: 0.0024, accuracy 99.7625 %\n",
      "===>> epoch: 395,  error: 0.0024, accuracy 99.7636 %\n",
      "===>> epoch: 396,  error: 0.0024, accuracy 99.7647 %\n",
      "===>> epoch: 397,  error: 0.0023, accuracy 99.7657 %\n",
      "===>> epoch: 398,  error: 0.0023, accuracy 99.7667 %\n",
      "===>> epoch: 399,  error: 0.0023, accuracy 99.7678 %\n",
      "===>> epoch: 400,  error: 0.0023, accuracy 99.7688 %\n",
      "===>> epoch: 401,  error: 0.0023, accuracy 99.7698 %\n",
      "===>> epoch: 402,  error: 0.0023, accuracy 99.7708 %\n",
      "===>> epoch: 403,  error: 0.0023, accuracy 99.7718 %\n",
      "===>> epoch: 404,  error: 0.0023, accuracy 99.7728 %\n",
      "===>> epoch: 405,  error: 0.0023, accuracy 99.7737 %\n",
      "===>> epoch: 406,  error: 0.0023, accuracy 99.7747 %\n",
      "===>> epoch: 407,  error: 0.0022, accuracy 99.7757 %\n",
      "===>> epoch: 408,  error: 0.0022, accuracy 99.7766 %\n",
      "===>> epoch: 409,  error: 0.0022, accuracy 99.7776 %\n",
      "===>> epoch: 410,  error: 0.0022, accuracy 99.7785 %\n",
      "===>> epoch: 411,  error: 0.0022, accuracy 99.7795 %\n",
      "===>> epoch: 412,  error: 0.0022, accuracy 99.7804 %\n",
      "===>> epoch: 413,  error: 0.0022, accuracy 99.7813 %\n",
      "===>> epoch: 414,  error: 0.0022, accuracy 99.7822 %\n",
      "===>> epoch: 415,  error: 0.0022, accuracy 99.7831 %\n",
      "===>> epoch: 416,  error: 0.0022, accuracy 99.7840 %\n",
      "===>> epoch: 417,  error: 0.0022, accuracy 99.7849 %\n",
      "===>> epoch: 418,  error: 0.0021, accuracy 99.7858 %\n",
      "===>> epoch: 419,  error: 0.0021, accuracy 99.7867 %\n",
      "===>> epoch: 420,  error: 0.0021, accuracy 99.7876 %\n",
      "===>> epoch: 421,  error: 0.0021, accuracy 99.7884 %\n",
      "===>> epoch: 422,  error: 0.0021, accuracy 99.7893 %\n",
      "===>> epoch: 423,  error: 0.0021, accuracy 99.7901 %\n",
      "===>> epoch: 424,  error: 0.0021, accuracy 99.7910 %\n",
      "===>> epoch: 425,  error: 0.0021, accuracy 99.7918 %\n",
      "===>> epoch: 426,  error: 0.0021, accuracy 99.7927 %\n",
      "===>> epoch: 427,  error: 0.0021, accuracy 99.7935 %\n",
      "===>> epoch: 428,  error: 0.0021, accuracy 99.7943 %\n",
      "===>> epoch: 429,  error: 0.0020, accuracy 99.7951 %\n",
      "===>> epoch: 430,  error: 0.0020, accuracy 99.7960 %\n",
      "===>> epoch: 431,  error: 0.0020, accuracy 99.7968 %\n",
      "===>> epoch: 432,  error: 0.0020, accuracy 99.7976 %\n",
      "===>> epoch: 433,  error: 0.0020, accuracy 99.7984 %\n",
      "===>> epoch: 434,  error: 0.0020, accuracy 99.7991 %\n",
      "===>> epoch: 435,  error: 0.0020, accuracy 99.7999 %\n",
      "===>> epoch: 436,  error: 0.0020, accuracy 99.8007 %\n",
      "===>> epoch: 437,  error: 0.0020, accuracy 99.8015 %\n",
      "===>> epoch: 438,  error: 0.0020, accuracy 99.8023 %\n",
      "===>> epoch: 439,  error: 0.0020, accuracy 99.8030 %\n",
      "===>> epoch: 440,  error: 0.0020, accuracy 99.8038 %\n",
      "===>> epoch: 441,  error: 0.0020, accuracy 99.8045 %\n",
      "===>> epoch: 442,  error: 0.0019, accuracy 99.8053 %\n",
      "===>> epoch: 443,  error: 0.0019, accuracy 99.8060 %\n",
      "===>> epoch: 444,  error: 0.0019, accuracy 99.8068 %\n",
      "===>> epoch: 445,  error: 0.0019, accuracy 99.8075 %\n",
      "===>> epoch: 446,  error: 0.0019, accuracy 99.8082 %\n",
      "===>> epoch: 447,  error: 0.0019, accuracy 99.8089 %\n",
      "===>> epoch: 448,  error: 0.0019, accuracy 99.8097 %\n",
      "===>> epoch: 449,  error: 0.0019, accuracy 99.8104 %\n",
      "===>> epoch: 450,  error: 0.0019, accuracy 99.8111 %\n",
      "===>> epoch: 451,  error: 0.0019, accuracy 99.8118 %\n",
      "===>> epoch: 452,  error: 0.0019, accuracy 99.8125 %\n",
      "===>> epoch: 453,  error: 0.0019, accuracy 99.8132 %\n",
      "===>> epoch: 454,  error: 0.0019, accuracy 99.8139 %\n",
      "===>> epoch: 455,  error: 0.0019, accuracy 99.8145 %\n",
      "===>> epoch: 456,  error: 0.0018, accuracy 99.8152 %\n",
      "===>> epoch: 457,  error: 0.0018, accuracy 99.8159 %\n",
      "===>> epoch: 458,  error: 0.0018, accuracy 99.8166 %\n",
      "===>> epoch: 459,  error: 0.0018, accuracy 99.8172 %\n",
      "===>> epoch: 460,  error: 0.0018, accuracy 99.8179 %\n",
      "===>> epoch: 461,  error: 0.0018, accuracy 99.8186 %\n",
      "===>> epoch: 462,  error: 0.0018, accuracy 99.8192 %\n",
      "===>> epoch: 463,  error: 0.0018, accuracy 99.8199 %\n",
      "===>> epoch: 464,  error: 0.0018, accuracy 99.8205 %\n",
      "===>> epoch: 465,  error: 0.0018, accuracy 99.8212 %\n",
      "===>> epoch: 466,  error: 0.0018, accuracy 99.8218 %\n",
      "===>> epoch: 467,  error: 0.0018, accuracy 99.8224 %\n",
      "===>> epoch: 468,  error: 0.0018, accuracy 99.8231 %\n",
      "===>> epoch: 469,  error: 0.0018, accuracy 99.8237 %\n",
      "===>> epoch: 470,  error: 0.0018, accuracy 99.8243 %\n",
      "===>> epoch: 471,  error: 0.0018, accuracy 99.8249 %\n",
      "===>> epoch: 472,  error: 0.0017, accuracy 99.8255 %\n",
      "===>> epoch: 473,  error: 0.0017, accuracy 99.8261 %\n",
      "===>> epoch: 474,  error: 0.0017, accuracy 99.8267 %\n",
      "===>> epoch: 475,  error: 0.0017, accuracy 99.8274 %\n",
      "===>> epoch: 476,  error: 0.0017, accuracy 99.8279 %\n",
      "===>> epoch: 477,  error: 0.0017, accuracy 99.8285 %\n",
      "===>> epoch: 478,  error: 0.0017, accuracy 99.8291 %\n",
      "===>> epoch: 479,  error: 0.0017, accuracy 99.8297 %\n",
      "===>> epoch: 480,  error: 0.0017, accuracy 99.8303 %\n",
      "===>> epoch: 481,  error: 0.0017, accuracy 99.8309 %\n",
      "===>> epoch: 482,  error: 0.0017, accuracy 99.8315 %\n",
      "===>> epoch: 483,  error: 0.0017, accuracy 99.8320 %\n",
      "===>> epoch: 484,  error: 0.0017, accuracy 99.8326 %\n",
      "===>> epoch: 485,  error: 0.0017, accuracy 99.8332 %\n",
      "===>> epoch: 486,  error: 0.0017, accuracy 99.8337 %\n",
      "===>> epoch: 487,  error: 0.0017, accuracy 99.8343 %\n",
      "===>> epoch: 488,  error: 0.0017, accuracy 99.8348 %\n",
      "===>> epoch: 489,  error: 0.0016, accuracy 99.8354 %\n",
      "===>> epoch: 490,  error: 0.0016, accuracy 99.8360 %\n",
      "===>> epoch: 491,  error: 0.0016, accuracy 99.8365 %\n",
      "===>> epoch: 492,  error: 0.0016, accuracy 99.8370 %\n",
      "===>> epoch: 493,  error: 0.0016, accuracy 99.8376 %\n",
      "===>> epoch: 494,  error: 0.0016, accuracy 99.8381 %\n",
      "===>> epoch: 495,  error: 0.0016, accuracy 99.8386 %\n",
      "===>> epoch: 496,  error: 0.0016, accuracy 99.8392 %\n",
      "===>> epoch: 497,  error: 0.0016, accuracy 99.8397 %\n",
      "===>> epoch: 498,  error: 0.0016, accuracy 99.8402 %\n",
      "===>> epoch: 499,  error: 0.0016, accuracy 99.8407 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "network = build_network(networks)\n",
    "\n",
    "errors, predictions, delta = train_model(X, y, network, lr=learning_rate, epoches=epoches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot figure to check errors\n",
    "\n",
    "# plt.plot(range(epoches), errors, color=\"green\", marker='o')\n",
    "\n",
    "# plt.plot(range(epoches), errors, marker='o',color='blue', markersize=3, linestyle='dashed',linewidth=1)\n",
    "# plt.xlabel(\"epoches\")\n",
    "# plt.ylabel(\"errors, J(theta)\")\n",
    "# plt.show()\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "ax.set_xlabel(\"epoches\")\n",
    "ax.set_ylabel(\"errors, J(theta)\")\n",
    "_ = ax.plot(range(epoches), errors, 'b.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(test_set, y, network):\n",
    "    \n",
    "    output = [test_set]\n",
    "#     predict = []\n",
    "    for layer in range(len(network)):\n",
    "        network[layer].forward(output[-1])\n",
    "        z = network[layer].output\n",
    "        a = activate_sigmoid(z)\n",
    "        output.append(a)\n",
    "    \n",
    "    # optimize of the output\n",
    "    output[-1][output[-1] > 0.5] = 1\n",
    "    output[-1][output[-1] <= 0.5] = 0   \n",
    "#     predict = output\n",
    "    \n",
    "#     for i in range(len(output[-1])):\n",
    "#         if output[-1][i] > 0.5:\n",
    "#             predict.append(1)\n",
    "#         else:\n",
    "#             predict.append(0)\n",
    "#     predict.append(1) for i in range(len(output[-1])) if output[-1][i] > 0.5 else predict.append(0)\n",
    "\n",
    "    error = loss_function_mse(output[-1], y)\n",
    "    return output[-1], error\n",
    "\n",
    "# output, predict, error = predict(test_set, y_set, network)\n",
    "output, error = predict(test_set, y_test_set, network)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected:     [1] \t Got ==> predicted(yhat): 1.0 \n",
      "\n",
      "expected:     [1] \t Got ==> predicted(yhat): 1.0 \n",
      "\n",
      "expected:     [0] \t Got ==> predicted(yhat): 0.0 \n",
      "\n",
      "expected:     [1] \t Got ==> predicted(yhat): 1.0 \n",
      "\n",
      "expected:     [1] \t Got ==> predicted(yhat): 1.0 \n",
      "\n",
      "expected:     [1] \t Got ==> predicted(yhat): 1.0 \n",
      "\n",
      "expected:     [0] \t Got ==> predicted(yhat): 1.0 \n",
      "\n",
      "expected:     [0] \t Got ==> predicted(yhat): 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(output.shape[0]):\n",
    "    print(\"expected:     {} \\t Got ==> predicted(yhat): {} \\n\".format(y_test_set[i], np.round(np.mean(output[i]),2)))\n",
    "#     print(\"accuracy: {}\".format(max(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python37264bitvenvvirtualenvc8215f58c15a47deba343d80afd29add"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
