{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression of Machine Learning with Gredicent Descent \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful links\n",
    "\n",
    "[1](https://christophm.github.io/interpretable-ml-book/preface-by-the-author.html)\n",
    "[2](https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Neuron networks class\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "# from lr_utils import load_dataset\n",
    "%matplotlib inline\n",
    "\n",
    "class NeuronNetworks:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function define\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def sigmoid_deriviative(z):\n",
    "    d_s = sigmoid(z)*(1 - sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a set random weights\n",
    "\n",
    "def initialize_with_zeros(m):\n",
    "    \"\"\" \n",
    "     This function creates a vector of zeros of shape (m, 1) for w and initializes b to 0.\n",
    "\n",
    "     Argument:\n",
    "     dim — size of the w vector we want (or number of parameters in this case)\n",
    "\n",
    "     Returns:\n",
    "     w — initialized vector of shape (dim, 1)\n",
    "     b — initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    w = np.zeros((m, 1))\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propagate function definition\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    ''' \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - training set, input data set\n",
    "    Y - target value (vectors) Y — true “label” vector\n",
    "    \n",
    "    Returns: \n",
    "    cost — negative log-likelihood cost for logistic regression\n",
    "    dw — gradient of the loss with respect to w, thus same shape as w\n",
    "    db — gradient of the loss with respect to b, thus same shape as b\n",
    "    '''\n",
    "    # the length of the input sets/training set\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward propagation by Activation function - sigmoid\n",
    "    A = sigmoid(np.dot(w.T, X)+ b) # compute activation function\n",
    "    cost = -(1/m)*(np.sum((Y*np.log(A)) + (1-Y) * np.log(1-A)))\n",
    "    \n",
    "    print(\"Inner Actication function value\", A)\n",
    "    # Backforward propagation by derivative of Activation function and output errors\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = (1/m)* np.dot(X, ((A-Y).T))\n",
    "    db = (1/m) * np.sum(A-Y)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Actication function value [[0.99987661 0.99999386]]\n",
      "Gradients \n",
      " dw: [[0.99993216]\n",
      " [1.99980262]] \t db: 0.49993523062470574\n",
      "Cost 6.000064773192205\n"
     ]
    }
   ],
   "source": [
    "w, b = np.array([[1], [2]]), 2\n",
    "X = np.array([[1,2], [3,4]])\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "print(\"Gradients \\n dw: {} \\t db: {}\".format(grads['dw'], grads['db']))\n",
    "print(\"Cost\", cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw  # need to broadcast\n",
    "        b = b - learning_rate * db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Actication function value [[0.99987661 0.99999386]]\n",
      "Inner Actication function value [[0.99986799 0.99999325]]\n",
      "Inner Actication function value [[0.99985877 0.99999258]]\n",
      "Inner Actication function value [[0.99984892 0.99999184]]\n",
      "Inner Actication function value [[0.99983837 0.99999103]]\n",
      "Inner Actication function value [[0.99982709 0.99999015]]\n",
      "Inner Actication function value [[0.99981501 0.99998917]]\n",
      "Inner Actication function value [[0.9998021 0.9999881]]\n",
      "Inner Actication function value [[0.99978829 0.99998692]]\n",
      "Inner Actication function value [[0.99977351 0.99998562]]\n",
      "Inner Actication function value [[0.9997577  0.99998419]]\n",
      "Inner Actication function value [[0.99974079 0.99998263]]\n",
      "Inner Actication function value [[0.9997227  0.99998091]]\n",
      "Inner Actication function value [[0.99970334 0.99997902]]\n",
      "Inner Actication function value [[0.99968264 0.99997694]]\n",
      "Inner Actication function value [[0.99966049 0.99997465]]\n",
      "Inner Actication function value [[0.9996368  0.99997214]]\n",
      "Inner Actication function value [[0.99961145 0.99996938]]\n",
      "Inner Actication function value [[0.99958434 0.99996634]]\n",
      "Inner Actication function value [[0.99955534 0.99996301]]\n",
      "Inner Actication function value [[0.99952431 0.99995935]]\n",
      "Inner Actication function value [[0.99949113 0.99995532]]\n",
      "Inner Actication function value [[0.99945563 0.99995089]]\n",
      "Inner Actication function value [[0.99941765 0.99994603]]\n",
      "Inner Actication function value [[0.99937703 0.99994068]]\n",
      "Inner Actication function value [[0.99933358 0.99993481]]\n",
      "Inner Actication function value [[0.9992871  0.99992835]]\n",
      "Inner Actication function value [[0.99923739 0.99992125]]\n",
      "Inner Actication function value [[0.99918422 0.99991345]]\n",
      "Inner Actication function value [[0.99912734 0.99990488]]\n",
      "Inner Actication function value [[0.99906651 0.99989546]]\n",
      "Inner Actication function value [[0.99900144 0.99988511]]\n",
      "Inner Actication function value [[0.99893184 0.99987374]]\n",
      "Inner Actication function value [[0.99885741 0.99986124]]\n",
      "Inner Actication function value [[0.9987778 0.9998475]]\n",
      "Inner Actication function value [[0.99869266 0.99983241]]\n",
      "Inner Actication function value [[0.9986016  0.99981582]]\n",
      "Inner Actication function value [[0.99850422 0.99979759]]\n",
      "Inner Actication function value [[0.99840007 0.99977756]]\n",
      "Inner Actication function value [[0.9982887  0.99975555]]\n",
      "Inner Actication function value [[0.9981696  0.99973137]]\n",
      "Inner Actication function value [[0.99804225 0.9997048 ]]\n",
      "Inner Actication function value [[0.99790607 0.9996756 ]]\n",
      "Inner Actication function value [[0.99776046 0.99964352]]\n",
      "Inner Actication function value [[0.99760477 0.99960828]]\n",
      "Inner Actication function value [[0.99743831 0.99956956]]\n",
      "Inner Actication function value [[0.99726034 0.99952702]]\n",
      "Inner Actication function value [[0.99707008 0.99948029]]\n",
      "Inner Actication function value [[0.99686669 0.99942895]]\n",
      "Inner Actication function value [[0.99664927 0.99937255]]\n",
      "Inner Actication function value [[0.99641687 0.99931061]]\n",
      "Inner Actication function value [[0.99616847 0.99924257]]\n",
      "Inner Actication function value [[0.99590299 0.99916783]]\n",
      "Inner Actication function value [[0.99561928 0.99908576]]\n",
      "Inner Actication function value [[0.9953161  0.99899562]]\n",
      "Inner Actication function value [[0.99499216 0.99889664]]\n",
      "Inner Actication function value [[0.99464605 0.99878796]]\n",
      "Inner Actication function value [[0.99427629 0.99866863]]\n",
      "Inner Actication function value [[0.99388132 0.99853762]]\n",
      "Inner Actication function value [[0.99345946 0.9983938 ]]\n",
      "Inner Actication function value [[0.99300893 0.99823594]]\n",
      "Inner Actication function value [[0.99252784 0.99806268]]\n",
      "Inner Actication function value [[0.99201419 0.99787254]]\n",
      "Inner Actication function value [[0.99146586 0.99766391]]\n",
      "Inner Actication function value [[0.9908806  0.99743501]]\n",
      "Inner Actication function value [[0.99025603 0.99718392]]\n",
      "Inner Actication function value [[0.98958962 0.99690854]]\n",
      "Inner Actication function value [[0.98887872 0.99660655]]\n",
      "Inner Actication function value [[0.98812051 0.99627546]]\n",
      "Inner Actication function value [[0.98731201 0.99591253]]\n",
      "Inner Actication function value [[0.98645011 0.9955148 ]]\n",
      "Inner Actication function value [[0.98553151 0.99507903]]\n",
      "Inner Actication function value [[0.98455275 0.99460169]]\n",
      "Inner Actication function value [[0.98351019 0.99407899]]\n",
      "Inner Actication function value [[0.98240003 0.99350678]]\n",
      "Inner Actication function value [[0.98121828 0.99288058]]\n",
      "Inner Actication function value [[0.97996077 0.99219554]]\n",
      "Inner Actication function value [[0.97862318 0.99144642]]\n",
      "Inner Actication function value [[0.97720099 0.99062757]]\n",
      "Inner Actication function value [[0.97568951 0.9897329 ]]\n",
      "Inner Actication function value [[0.97408389 0.98875588]]\n",
      "Inner Actication function value [[0.97237914 0.98768948]]\n",
      "Inner Actication function value [[0.97057011 0.98652619]]\n",
      "Inner Actication function value [[0.96865152 0.98525798]]\n",
      "Inner Actication function value [[0.96661798 0.98387631]]\n",
      "Inner Actication function value [[0.96446401 0.98237211]]\n",
      "Inner Actication function value [[0.96218408 0.98073575]]\n",
      "Inner Actication function value [[0.95977259 0.97895711]]\n",
      "Inner Actication function value [[0.95722397 0.97702553]]\n",
      "Inner Actication function value [[0.95453266 0.97492985]]\n",
      "Inner Actication function value [[0.9516932  0.97265846]]\n",
      "Inner Actication function value [[0.94870026 0.97019932]]\n",
      "Inner Actication function value [[0.94554867 0.96754003]]\n",
      "Inner Actication function value [[0.94223352 0.9646679 ]]\n",
      "Inner Actication function value [[0.9387502  0.96157005]]\n",
      "Inner Actication function value [[0.93509447 0.95823347]]\n",
      "Inner Actication function value [[0.93126253 0.95464522]]\n",
      "Inner Actication function value [[0.92725111 0.95079251]]\n",
      "Inner Actication function value [[0.92305751 0.94666289]]\n",
      "Inner Actication function value [[0.91867973 0.94224442]]\n",
      "Params {'w': array([[0.1124579 ],\n",
      "       [0.23106775]]), 'b': 1.5593049248448891}\n",
      "grads mini cost {'dw': array([[0.90158428],\n",
      "       [1.76250842]]), 'db': 0.4304620716786828}\n",
      "real cost [6.000064773192205]\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs_history = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "print(\"Params\", params)\n",
    "print(\"grads mini cost\", grads)\n",
    "print(\"real cost\", costs_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with output weights and bias\n",
    "\n",
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "     Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "\n",
    "     Arguments:\n",
    "     w — weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "     b — bias, a scalar\n",
    "     X — data of size (num_px * num_px * 3, number of examples)\n",
    "\n",
    "     Returns:\n",
    "     Y_prediction — a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m)) # create empty array with m dimentions\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        Y_prediction[0,i] = 1 if A[0, i] > 0.5 else 0\n",
    "#     pass\n",
    "    \n",
    "    return Y_prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python37264bitvenvvirtualenvc8215f58c15a47deba343d80afd29add"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
