{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inputs = np.array([[1,2],[1,3],[1,4],[1,5],[5,2],[3,1],[2,1],[1,1]])\n",
    "test_set = [[6,1],[-1, -2],[-2, 3],[3, 2],[3,2], [7,1], [-1, -2],[1,1]]\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 0]])\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 0]]).reshape(y.shape[1],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerDesen:\n",
    "    def __init__(self, n_inputs, n_neuron):\n",
    "        self.layer_n = None\n",
    "        self.n_neuron = n_neuron\n",
    "        self.weights = np.random.randn(n_inputs, n_neuron)\n",
    "        self.biases = np.zeros((1, n_neuron))\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases   \n",
    "    def __repr__(self):\n",
    "        return \"layer: {}th, neurons:{}, weights: {}, biases:{}\\n\".format(self.layer_n, self.n_neuron, self.weights, self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inputs.shape[0]\n",
    "n_inputs = X_inputs.shape[1]\n",
    "n_neuron = 1\n",
    "print(X_inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = LayerDesen(n_inputs, n_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1.forward(X_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "\n",
    "def activate_sigmoid(inputs):\n",
    "    return 1.0/(1.0+np.exp(-inputs))\n",
    "\n",
    "def der_sigmoid(inputs):\n",
    "    return inputs * (1 - inputs)\n",
    "\n",
    "def activate_ReLU(inputs):\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "def loss_function_mse(predict, target):\n",
    "    return np.mean((predict - target)**2)\n",
    "def der_loss_function_mse(predict, target):\n",
    "    return predict - target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerDesen:\n",
    "    def __init__(self, n_inputs, n_neuron):\n",
    "        self.layer_n = None\n",
    "        self.n_neuron = n_neuron\n",
    "        self.weights = np.random.randn(n_inputs, n_neuron)\n",
    "        self.biases = np.zeros((1, n_neuron))\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases   \n",
    "    def __repr__(self):\n",
    "        return \"layer: {}th, neurons:{}, weights: {}, biases:{}\\n\".format(self.layer_n, self.n_neuron, self.weights, self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial network model\n",
    "def build_network(network_list=[2,1,2]):\n",
    "    \n",
    "    # network_list = [2,1,2], 2, inputs, 1, hidden1-1, output, 2\n",
    "    network = []\n",
    "    if not isinstance(network_list, list):\n",
    "        return None\n",
    "    \n",
    "    # build network\n",
    "    for i in range(len(network_list)-1 ):\n",
    "        layer = LayerDesen(network_list[i],network_list[i+1])\n",
    "        layer.layer_n = i+1\n",
    "        network.append(layer)\n",
    "    \n",
    "    return network\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[layer: 1th, neurons:4, weights: [[ 0.89655468  0.62548483  1.25514105  1.08806729]\n",
       "  [-0.48233206  0.38531321  1.0864869  -0.02635608]], biases:[[0. 0. 0. 0.]],\n",
       " layer: 2th, neurons:2, weights: [[-0.44973512  0.53279961]\n",
       "  [-2.02247809 -0.18596922]\n",
       "  [-0.05596212 -0.1004956 ]\n",
       "  [-1.76899319 -0.21766855]], biases:[[0. 0.]],\n",
       " layer: 3th, neurons:1, weights: [[-1.32695921]\n",
       "  [ 1.79530549]], biases:[[0.]]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build network with Weights\n",
    "networks = [2,4,2,1]\n",
    "network = build_network(networks)\n",
    "\n",
    "network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation over network\n",
    "\n",
    "# layer1 = network[0]\n",
    "# z1 = np.dot(X_inputs, layer1.weights) + layer1.biases\n",
    "# a = activate_sigmoid(z1)\n",
    "\n",
    "# output = [a]\n",
    "output = [X_inputs]\n",
    "for layer in range(len(network)):\n",
    "#     z = np.dot(output[-1], network[layer].weights) + network[layer].biases\n",
    "    network[layer].forward(output[-1])\n",
    "    z = network[layer].output\n",
    "    a = activate_sigmoid(z)\n",
    "#     print(output)\n",
    "    output.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6853603 ],\n",
       "       [0.67879068],\n",
       "       [0.67306077],\n",
       "       [0.66851042],\n",
       "       [0.70823405],\n",
       "       [0.70573122],\n",
       "       [0.70104563],\n",
       "       [0.69222602]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes (8, 2)\n",
      "shapes (8, 4)\n",
      "shapes (8, 2)\n",
      "shapes (8, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ print(\"shapes\", o.shape) for o in output ]\n",
    "# X =  np.round(np.random.randn(20,2),3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back-propagation for last layer\n",
    "lr = 0.01\n",
    "error = output[-1] - y\n",
    "\n",
    "# delta = output.T.dot(error)\n",
    "\n",
    "network[-1].biases = network[-1].biases - error.mean() * lr\n",
    "network[-1].weights = network[-1].weights - (output[-1].T.dot(error)) * lr\n",
    "\n",
    "\n",
    "# network[-1].biases = red_neuronal[-1].b - x.mean() * 0.01\n",
    "# red_neuronal[-1].W = red_neuronal[-1].W - (output[-1].T @ x) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[layer: 1th, neurons:4, weights: [[ 0.89655468  0.62548483  1.25514105  1.08806729]\n",
       "  [-0.48233206  0.38531321  1.0864869  -0.02635608]], biases:[[0. 0. 0. 0.]],\n",
       " layer: 2th, neurons:2, weights: [[-0.44973512  0.53279961]\n",
       "  [-2.02247809 -0.18596922]\n",
       "  [-0.05596212 -0.1004956 ]\n",
       "  [-1.76899319 -0.21766855]], biases:[[0. 0.]],\n",
       " layer: 3th, neurons:1, weights: [[-1.34381597]\n",
       "  [ 1.77844873]], biases:[[-0.0031412]]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> network 2 layer, weights => [[-1.3450556 ]\n",
      " [ 1.76650739]] bias =>[[-0.00661285]] updated\n",
      "===> network 1 layer, weights => [[-0.4492278   0.5302596 ]\n",
      " [-2.02115005 -0.19592684]\n",
      " [-0.05441392 -0.11223195]\n",
      " [-1.7678699  -0.22566682]] bias =>[[ 0.00019969 -0.00151618]] updated\n",
      "===> network 0 layer, weights => [[ 0.89510887  0.62533461  1.25516955  1.08808005]\n",
      " [-0.487651    0.38508654  1.08654179 -0.02615769]] bias =>[[-2.24414302e-04 -1.42732641e-05  4.26727109e-06  5.00678113e-06]] updated\n"
     ]
    }
   ],
   "source": [
    "lr = 0.05\n",
    "# back = list(range(len(output)-1))\n",
    "# back.reverse()\n",
    "# back-propagation for rest of layers and updates weights and biases\n",
    "delta = []\n",
    "back = list(range(len(output)-1))\n",
    "back.reverse()\n",
    "\n",
    "for i in back:\n",
    "    # get one of the output of next layer\n",
    "    next_a = output[i+1]\n",
    "#     print(next_a)\n",
    "    if i == back[0]:\n",
    "#         print(\"first run delta\",delta)\n",
    "        # calculate the delta between current and next layer\n",
    "        # error * der_sigmoid(previous_output), gredicent here\n",
    "#         print(\"next a\", next_a)\n",
    "        x = (next_a - y) * der_sigmoid(next_a)\n",
    "        delta.append(x)\n",
    "#         print(\"delta current is: -->{}\\n\".format(delta))\n",
    "    else:\n",
    "        # delta = last delta.T.dot(last weights)*der_sigmoid_with_output\n",
    "#         print(\"currnet delta is: -->\", delta[-1])\n",
    "#         print(\"previous weights is: -->\", previous_W)\n",
    "\n",
    "        x = delta[-1] @ previous_W * der_sigmoid(next_a)\n",
    "        delta.append(x)\n",
    "    \n",
    "    # store tmp W for next layer previous compute\n",
    "    previous_W = network[i].weights.transpose()\n",
    "    \n",
    "    # gradicent descent for update weights\n",
    "    network[i].biases = network[i].biases - np.mean(delta[-1], axis = 0, keepdims = True) * lr\n",
    "#     print(\"output Matrix shape {}, delta Matrix shape {}\".format(output[i].shape, delta[-1].shape))\n",
    "#     print(\"===> output.matrix{} dot delta[-1] {}.matrix\".format(output[i], delta[-1]))\n",
    "    network[i].weights = network[i].weights - output[i].T @ delta[-1] * lr\n",
    "    \n",
    "    print(\"===> network {} layer, weights => {} bias =>{} updated\".format(i, network[i].weights, network[i].biases))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[layer: 1th, neurons:4, weights: [[ 0.89510887  0.62533461  1.25516955  1.08808005]\n",
       "  [-0.487651    0.38508654  1.08654179 -0.02615769]], biases:[[-2.24414302e-04 -1.42732641e-05  4.26727109e-06  5.00678113e-06]],\n",
       " layer: 2th, neurons:2, weights: [[-0.4492278   0.5302596 ]\n",
       "  [-2.02115005 -0.19592684]\n",
       "  [-0.05441392 -0.11223195]\n",
       "  [-1.7678699  -0.22566682]], biases:[[ 0.00019969 -0.00151618]],\n",
       " layer: 3th, neurons:1, weights: [[-1.3450556 ]\n",
       "  [ 1.76650739]], biases:[[-0.00661285]]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3213330714813073\n",
      "Estimation: [[0.6853603 ]\n",
      " [0.67879068]\n",
      " [0.67306077]\n",
      " [0.66851042]\n",
      " [0.70823405]\n",
      " [0.70573122]\n",
      " [0.70104563]\n",
      " [0.69222602]]\n"
     ]
    }
   ],
   "source": [
    "print('MSE: ' + str(loss_function_mse(output[-1],y)))\n",
    "print('Estimation: ' + str(output[-1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our model\n",
    "\n",
    "def build_model(X, y, network, lr=0.01):\n",
    "    \n",
    "    # forward propagation\n",
    "    output = [X]\n",
    "#     print(X)\n",
    "    for layer in range(len(network)):\n",
    "        network[layer].forward(output[-1])\n",
    "        z = network[layer].output\n",
    "        a = activate_sigmoid(z)\n",
    "        output.append(a)\n",
    "    \n",
    "    # backforward propagation for output and errors\n",
    "    delta = []\n",
    "    back = list(range(len(output)-1))\n",
    "    back.reverse()\n",
    "\n",
    "    for layer in back:\n",
    "        # get one of the output of next layer\n",
    "        next_a = output[layer+1]\n",
    "        if layer == back[0]:\n",
    "            x = (next_a - y) * der_sigmoid(next_a)\n",
    "            delta.append(x)\n",
    "        else:\n",
    "            x = delta[-1] @ previous_W * der_sigmoid(next_a)\n",
    "            delta.append(x)\n",
    "        # store tmp W for next layer previous compute\n",
    "        previous_W = network[layer].weights.transpose()\n",
    "\n",
    "        # gradicent descent for update weights\n",
    "        network[layer].biases = network[layer].biases - np.mean(delta[-1], axis = 0, keepdims = True) * lr\n",
    "#         print(\"output Matrix shape {}, delta Matrix shape {}\".format(output[i].shape, delta[-1].shape))\n",
    "#         print(\"===> output.matrix{} dot delta[-1] {}.matrix\".format(output[i], delta[-1]))\n",
    "        network[layer].weights = network[layer].weights - output[layer].T @ delta[-1] * lr\n",
    "#       print(\"===> network {} layer, weights => {} bias =>{} updated\".format(i, network[i].weights, network[i].biases))\n",
    "    \n",
    "    return output[-1]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64331945],\n",
       "       [0.64571144],\n",
       "       [0.64634581],\n",
       "       [0.6462019 ],\n",
       "       [0.63540217],\n",
       "       [0.62882065],\n",
       "       [0.62928512],\n",
       "       [0.63794497]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_model(X_inputs, y, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, network, lr=0.01, epoches=100):\n",
    "    errors = []\n",
    "    predictions = []\n",
    "    for i in range(epoches):\n",
    "        prediction = build_model(X, y, network, lr=lr)\n",
    "        predictions.append(prediction)\n",
    "        error = loss_function_mse(prediction, y)\n",
    "        print(\"===>> epoches {},  accuracy {:.4f} %\".format(i , (1- error)*100))\n",
    "        errors.append(error)\n",
    "        \n",
    "    return errors, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2],[1,3],[1,4],[1,5],[5,2],[3,1],[2,1],[1,1]])\n",
    "test_set = [[6,1],[-1, -2],[-2, 3],[3, 2]]\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 0]])\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 0]]).reshape(y.shape[1],1)\n",
    "\n",
    "# Build network with Weights\n",
    "learning_rate = 0.5\n",
    "epoches = 300\n",
    "\n",
    "networks = [2,4,2,1]\n",
    "network = build_network(networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>> epoches 0,  accuracy 67.2105 %\n",
      "===>> epoches 1,  accuracy 71.7373 %\n",
      "===>> epoches 2,  accuracy 74.6789 %\n",
      "===>> epoches 3,  accuracy 76.3611 %\n",
      "===>> epoches 4,  accuracy 77.3575 %\n",
      "===>> epoches 5,  accuracy 78.0378 %\n",
      "===>> epoches 6,  accuracy 78.5766 %\n",
      "===>> epoches 7,  accuracy 79.0418 %\n",
      "===>> epoches 8,  accuracy 79.4564 %\n",
      "===>> epoches 9,  accuracy 79.8311 %\n",
      "===>> epoches 10,  accuracy 80.1754 %\n",
      "===>> epoches 11,  accuracy 80.4988 %\n",
      "===>> epoches 12,  accuracy 80.8094 %\n",
      "===>> epoches 13,  accuracy 81.1136 %\n",
      "===>> epoches 14,  accuracy 81.4159 %\n",
      "===>> epoches 15,  accuracy 81.7196 %\n",
      "===>> epoches 16,  accuracy 82.0270 %\n",
      "===>> epoches 17,  accuracy 82.3394 %\n",
      "===>> epoches 18,  accuracy 82.6578 %\n",
      "===>> epoches 19,  accuracy 82.9826 %\n",
      "===>> epoches 20,  accuracy 83.3136 %\n",
      "===>> epoches 21,  accuracy 83.6506 %\n",
      "===>> epoches 22,  accuracy 83.9929 %\n",
      "===>> epoches 23,  accuracy 84.3396 %\n",
      "===>> epoches 24,  accuracy 84.6895 %\n",
      "===>> epoches 25,  accuracy 85.0414 %\n",
      "===>> epoches 26,  accuracy 85.3939 %\n",
      "===>> epoches 27,  accuracy 85.7456 %\n",
      "===>> epoches 28,  accuracy 86.0953 %\n",
      "===>> epoches 29,  accuracy 86.4417 %\n",
      "===>> epoches 30,  accuracy 86.7841 %\n",
      "===>> epoches 31,  accuracy 87.1214 %\n",
      "===>> epoches 32,  accuracy 87.4531 %\n",
      "===>> epoches 33,  accuracy 87.7787 %\n",
      "===>> epoches 34,  accuracy 88.0978 %\n",
      "===>> epoches 35,  accuracy 88.4102 %\n",
      "===>> epoches 36,  accuracy 88.7157 %\n",
      "===>> epoches 37,  accuracy 89.0143 %\n",
      "===>> epoches 38,  accuracy 89.3057 %\n",
      "===>> epoches 39,  accuracy 89.5901 %\n",
      "===>> epoches 40,  accuracy 89.8674 %\n",
      "===>> epoches 41,  accuracy 90.1376 %\n",
      "===>> epoches 42,  accuracy 90.4007 %\n",
      "===>> epoches 43,  accuracy 90.6568 %\n",
      "===>> epoches 44,  accuracy 90.9059 %\n",
      "===>> epoches 45,  accuracy 91.1481 %\n",
      "===>> epoches 46,  accuracy 91.3835 %\n",
      "===>> epoches 47,  accuracy 91.6122 %\n",
      "===>> epoches 48,  accuracy 91.8341 %\n",
      "===>> epoches 49,  accuracy 92.0496 %\n",
      "===>> epoches 50,  accuracy 92.2586 %\n",
      "===>> epoches 51,  accuracy 92.4613 %\n",
      "===>> epoches 52,  accuracy 92.6578 %\n",
      "===>> epoches 53,  accuracy 92.8482 %\n",
      "===>> epoches 54,  accuracy 93.0327 %\n",
      "===>> epoches 55,  accuracy 93.2114 %\n",
      "===>> epoches 56,  accuracy 93.3845 %\n",
      "===>> epoches 57,  accuracy 93.5521 %\n",
      "===>> epoches 58,  accuracy 93.7143 %\n",
      "===>> epoches 59,  accuracy 93.8713 %\n",
      "===>> epoches 60,  accuracy 94.0233 %\n",
      "===>> epoches 61,  accuracy 94.1704 %\n",
      "===>> epoches 62,  accuracy 94.3128 %\n",
      "===>> epoches 63,  accuracy 94.4506 %\n",
      "===>> epoches 64,  accuracy 94.5839 %\n",
      "===>> epoches 65,  accuracy 94.7129 %\n",
      "===>> epoches 66,  accuracy 94.8378 %\n",
      "===>> epoches 67,  accuracy 94.9587 %\n",
      "===>> epoches 68,  accuracy 95.0756 %\n",
      "===>> epoches 69,  accuracy 95.1889 %\n",
      "===>> epoches 70,  accuracy 95.2985 %\n",
      "===>> epoches 71,  accuracy 95.4046 %\n",
      "===>> epoches 72,  accuracy 95.5074 %\n",
      "===>> epoches 73,  accuracy 95.6069 %\n",
      "===>> epoches 74,  accuracy 95.7034 %\n",
      "===>> epoches 75,  accuracy 95.7968 %\n",
      "===>> epoches 76,  accuracy 95.8873 %\n",
      "===>> epoches 77,  accuracy 95.9750 %\n",
      "===>> epoches 78,  accuracy 96.0600 %\n",
      "===>> epoches 79,  accuracy 96.1424 %\n",
      "===>> epoches 80,  accuracy 96.2224 %\n",
      "===>> epoches 81,  accuracy 96.2999 %\n",
      "===>> epoches 82,  accuracy 96.3751 %\n",
      "===>> epoches 83,  accuracy 96.4480 %\n",
      "===>> epoches 84,  accuracy 96.5188 %\n",
      "===>> epoches 85,  accuracy 96.5875 %\n",
      "===>> epoches 86,  accuracy 96.6543 %\n",
      "===>> epoches 87,  accuracy 96.7191 %\n",
      "===>> epoches 88,  accuracy 96.7820 %\n",
      "===>> epoches 89,  accuracy 96.8431 %\n",
      "===>> epoches 90,  accuracy 96.9026 %\n",
      "===>> epoches 91,  accuracy 96.9603 %\n",
      "===>> epoches 92,  accuracy 97.0164 %\n",
      "===>> epoches 93,  accuracy 97.0710 %\n",
      "===>> epoches 94,  accuracy 97.1241 %\n",
      "===>> epoches 95,  accuracy 97.1757 %\n",
      "===>> epoches 96,  accuracy 97.2260 %\n",
      "===>> epoches 97,  accuracy 97.2749 %\n",
      "===>> epoches 98,  accuracy 97.3225 %\n",
      "===>> epoches 99,  accuracy 97.3688 %\n",
      "===>> epoches 100,  accuracy 97.4139 %\n",
      "===>> epoches 101,  accuracy 97.4579 %\n",
      "===>> epoches 102,  accuracy 97.5007 %\n",
      "===>> epoches 103,  accuracy 97.5424 %\n",
      "===>> epoches 104,  accuracy 97.5830 %\n",
      "===>> epoches 105,  accuracy 97.6227 %\n",
      "===>> epoches 106,  accuracy 97.6613 %\n",
      "===>> epoches 107,  accuracy 97.6990 %\n",
      "===>> epoches 108,  accuracy 97.7357 %\n",
      "===>> epoches 109,  accuracy 97.7716 %\n",
      "===>> epoches 110,  accuracy 97.8066 %\n",
      "===>> epoches 111,  accuracy 97.8407 %\n",
      "===>> epoches 112,  accuracy 97.8740 %\n",
      "===>> epoches 113,  accuracy 97.9065 %\n",
      "===>> epoches 114,  accuracy 97.9383 %\n",
      "===>> epoches 115,  accuracy 97.9693 %\n",
      "===>> epoches 116,  accuracy 97.9996 %\n",
      "===>> epoches 117,  accuracy 98.0292 %\n",
      "===>> epoches 118,  accuracy 98.0581 %\n",
      "===>> epoches 119,  accuracy 98.0864 %\n",
      "===>> epoches 120,  accuracy 98.1140 %\n",
      "===>> epoches 121,  accuracy 98.1410 %\n",
      "===>> epoches 122,  accuracy 98.1674 %\n",
      "===>> epoches 123,  accuracy 98.1933 %\n",
      "===>> epoches 124,  accuracy 98.2185 %\n",
      "===>> epoches 125,  accuracy 98.2432 %\n",
      "===>> epoches 126,  accuracy 98.2674 %\n",
      "===>> epoches 127,  accuracy 98.2911 %\n",
      "===>> epoches 128,  accuracy 98.3142 %\n",
      "===>> epoches 129,  accuracy 98.3369 %\n",
      "===>> epoches 130,  accuracy 98.3591 %\n",
      "===>> epoches 131,  accuracy 98.3808 %\n",
      "===>> epoches 132,  accuracy 98.4020 %\n",
      "===>> epoches 133,  accuracy 98.4229 %\n",
      "===>> epoches 134,  accuracy 98.4433 %\n",
      "===>> epoches 135,  accuracy 98.4633 %\n",
      "===>> epoches 136,  accuracy 98.4829 %\n",
      "===>> epoches 137,  accuracy 98.5021 %\n",
      "===>> epoches 138,  accuracy 98.5209 %\n",
      "===>> epoches 139,  accuracy 98.5393 %\n",
      "===>> epoches 140,  accuracy 98.5574 %\n",
      "===>> epoches 141,  accuracy 98.5751 %\n",
      "===>> epoches 142,  accuracy 98.5925 %\n",
      "===>> epoches 143,  accuracy 98.6096 %\n",
      "===>> epoches 144,  accuracy 98.6263 %\n",
      "===>> epoches 145,  accuracy 98.6427 %\n",
      "===>> epoches 146,  accuracy 98.6588 %\n",
      "===>> epoches 147,  accuracy 98.6746 %\n",
      "===>> epoches 148,  accuracy 98.6901 %\n",
      "===>> epoches 149,  accuracy 98.7053 %\n",
      "===>> epoches 150,  accuracy 98.7202 %\n",
      "===>> epoches 151,  accuracy 98.7349 %\n",
      "===>> epoches 152,  accuracy 98.7493 %\n",
      "===>> epoches 153,  accuracy 98.7634 %\n",
      "===>> epoches 154,  accuracy 98.7773 %\n",
      "===>> epoches 155,  accuracy 98.7909 %\n",
      "===>> epoches 156,  accuracy 98.8043 %\n",
      "===>> epoches 157,  accuracy 98.8174 %\n",
      "===>> epoches 158,  accuracy 98.8303 %\n",
      "===>> epoches 159,  accuracy 98.8430 %\n",
      "===>> epoches 160,  accuracy 98.8555 %\n",
      "===>> epoches 161,  accuracy 98.8678 %\n",
      "===>> epoches 162,  accuracy 98.8798 %\n",
      "===>> epoches 163,  accuracy 98.8917 %\n",
      "===>> epoches 164,  accuracy 98.9033 %\n",
      "===>> epoches 165,  accuracy 98.9147 %\n",
      "===>> epoches 166,  accuracy 98.9260 %\n",
      "===>> epoches 167,  accuracy 98.9371 %\n",
      "===>> epoches 168,  accuracy 98.9479 %\n",
      "===>> epoches 169,  accuracy 98.9586 %\n",
      "===>> epoches 170,  accuracy 98.9692 %\n",
      "===>> epoches 171,  accuracy 98.9795 %\n",
      "===>> epoches 172,  accuracy 98.9897 %\n",
      "===>> epoches 173,  accuracy 98.9997 %\n",
      "===>> epoches 174,  accuracy 99.0096 %\n",
      "===>> epoches 175,  accuracy 99.0193 %\n",
      "===>> epoches 176,  accuracy 99.0289 %\n",
      "===>> epoches 177,  accuracy 99.0383 %\n",
      "===>> epoches 178,  accuracy 99.0475 %\n",
      "===>> epoches 179,  accuracy 99.0566 %\n",
      "===>> epoches 180,  accuracy 99.0656 %\n",
      "===>> epoches 181,  accuracy 99.0745 %\n",
      "===>> epoches 182,  accuracy 99.0831 %\n",
      "===>> epoches 183,  accuracy 99.0917 %\n",
      "===>> epoches 184,  accuracy 99.1001 %\n",
      "===>> epoches 185,  accuracy 99.1085 %\n",
      "===>> epoches 186,  accuracy 99.1166 %\n",
      "===>> epoches 187,  accuracy 99.1247 %\n",
      "===>> epoches 188,  accuracy 99.1326 %\n",
      "===>> epoches 189,  accuracy 99.1405 %\n",
      "===>> epoches 190,  accuracy 99.1482 %\n",
      "===>> epoches 191,  accuracy 99.1558 %\n",
      "===>> epoches 192,  accuracy 99.1633 %\n",
      "===>> epoches 193,  accuracy 99.1706 %\n",
      "===>> epoches 194,  accuracy 99.1779 %\n",
      "===>> epoches 195,  accuracy 99.1851 %\n",
      "===>> epoches 196,  accuracy 99.1921 %\n",
      "===>> epoches 197,  accuracy 99.1991 %\n",
      "===>> epoches 198,  accuracy 99.2060 %\n",
      "===>> epoches 199,  accuracy 99.2128 %\n",
      "===>> epoches 200,  accuracy 99.2194 %\n",
      "===>> epoches 201,  accuracy 99.2260 %\n",
      "===>> epoches 202,  accuracy 99.2325 %\n",
      "===>> epoches 203,  accuracy 99.2389 %\n",
      "===>> epoches 204,  accuracy 99.2452 %\n",
      "===>> epoches 205,  accuracy 99.2515 %\n",
      "===>> epoches 206,  accuracy 99.2576 %\n",
      "===>> epoches 207,  accuracy 99.2637 %\n",
      "===>> epoches 208,  accuracy 99.2696 %\n",
      "===>> epoches 209,  accuracy 99.2756 %\n",
      "===>> epoches 210,  accuracy 99.2814 %\n",
      "===>> epoches 211,  accuracy 99.2871 %\n",
      "===>> epoches 212,  accuracy 99.2928 %\n",
      "===>> epoches 213,  accuracy 99.2984 %\n",
      "===>> epoches 214,  accuracy 99.3039 %\n",
      "===>> epoches 215,  accuracy 99.3094 %\n",
      "===>> epoches 216,  accuracy 99.3147 %\n",
      "===>> epoches 217,  accuracy 99.3201 %\n",
      "===>> epoches 218,  accuracy 99.3253 %\n",
      "===>> epoches 219,  accuracy 99.3305 %\n",
      "===>> epoches 220,  accuracy 99.3356 %\n",
      "===>> epoches 221,  accuracy 99.3406 %\n",
      "===>> epoches 222,  accuracy 99.3456 %\n",
      "===>> epoches 223,  accuracy 99.3505 %\n",
      "===>> epoches 224,  accuracy 99.3554 %\n",
      "===>> epoches 225,  accuracy 99.3602 %\n",
      "===>> epoches 226,  accuracy 99.3649 %\n",
      "===>> epoches 227,  accuracy 99.3696 %\n",
      "===>> epoches 228,  accuracy 99.3742 %\n",
      "===>> epoches 229,  accuracy 99.3788 %\n",
      "===>> epoches 230,  accuracy 99.3833 %\n",
      "===>> epoches 231,  accuracy 99.3878 %\n",
      "===>> epoches 232,  accuracy 99.3922 %\n",
      "===>> epoches 233,  accuracy 99.3965 %\n",
      "===>> epoches 234,  accuracy 99.4008 %\n",
      "===>> epoches 235,  accuracy 99.4051 %\n",
      "===>> epoches 236,  accuracy 99.4093 %\n",
      "===>> epoches 237,  accuracy 99.4134 %\n",
      "===>> epoches 238,  accuracy 99.4175 %\n",
      "===>> epoches 239,  accuracy 99.4216 %\n",
      "===>> epoches 240,  accuracy 99.4256 %\n",
      "===>> epoches 241,  accuracy 99.4296 %\n",
      "===>> epoches 242,  accuracy 99.4335 %\n",
      "===>> epoches 243,  accuracy 99.4373 %\n",
      "===>> epoches 244,  accuracy 99.4412 %\n",
      "===>> epoches 245,  accuracy 99.4450 %\n",
      "===>> epoches 246,  accuracy 99.4487 %\n",
      "===>> epoches 247,  accuracy 99.4524 %\n",
      "===>> epoches 248,  accuracy 99.4560 %\n",
      "===>> epoches 249,  accuracy 99.4597 %\n",
      "===>> epoches 250,  accuracy 99.4632 %\n",
      "===>> epoches 251,  accuracy 99.4668 %\n",
      "===>> epoches 252,  accuracy 99.4703 %\n",
      "===>> epoches 253,  accuracy 99.4737 %\n",
      "===>> epoches 254,  accuracy 99.4771 %\n",
      "===>> epoches 255,  accuracy 99.4805 %\n",
      "===>> epoches 256,  accuracy 99.4839 %\n",
      "===>> epoches 257,  accuracy 99.4872 %\n",
      "===>> epoches 258,  accuracy 99.4905 %\n",
      "===>> epoches 259,  accuracy 99.4937 %\n",
      "===>> epoches 260,  accuracy 99.4969 %\n",
      "===>> epoches 261,  accuracy 99.5001 %\n",
      "===>> epoches 262,  accuracy 99.5032 %\n",
      "===>> epoches 263,  accuracy 99.5063 %\n",
      "===>> epoches 264,  accuracy 99.5094 %\n",
      "===>> epoches 265,  accuracy 99.5124 %\n",
      "===>> epoches 266,  accuracy 99.5154 %\n",
      "===>> epoches 267,  accuracy 99.5184 %\n",
      "===>> epoches 268,  accuracy 99.5213 %\n",
      "===>> epoches 269,  accuracy 99.5243 %\n",
      "===>> epoches 270,  accuracy 99.5271 %\n",
      "===>> epoches 271,  accuracy 99.5300 %\n",
      "===>> epoches 272,  accuracy 99.5328 %\n",
      "===>> epoches 273,  accuracy 99.5356 %\n",
      "===>> epoches 274,  accuracy 99.5384 %\n",
      "===>> epoches 275,  accuracy 99.5411 %\n",
      "===>> epoches 276,  accuracy 99.5438 %\n",
      "===>> epoches 277,  accuracy 99.5465 %\n",
      "===>> epoches 278,  accuracy 99.5492 %\n",
      "===>> epoches 279,  accuracy 99.5518 %\n",
      "===>> epoches 280,  accuracy 99.5544 %\n",
      "===>> epoches 281,  accuracy 99.5570 %\n",
      "===>> epoches 282,  accuracy 99.5596 %\n",
      "===>> epoches 283,  accuracy 99.5621 %\n",
      "===>> epoches 284,  accuracy 99.5646 %\n",
      "===>> epoches 285,  accuracy 99.5671 %\n",
      "===>> epoches 286,  accuracy 99.5695 %\n",
      "===>> epoches 287,  accuracy 99.5720 %\n",
      "===>> epoches 288,  accuracy 99.5744 %\n",
      "===>> epoches 289,  accuracy 99.5768 %\n",
      "===>> epoches 290,  accuracy 99.5791 %\n",
      "===>> epoches 291,  accuracy 99.5815 %\n",
      "===>> epoches 292,  accuracy 99.5838 %\n",
      "===>> epoches 293,  accuracy 99.5861 %\n",
      "===>> epoches 294,  accuracy 99.5884 %\n",
      "===>> epoches 295,  accuracy 99.5906 %\n",
      "===>> epoches 296,  accuracy 99.5929 %\n",
      "===>> epoches 297,  accuracy 99.5951 %\n",
      "===>> epoches 298,  accuracy 99.5973 %\n",
      "===>> epoches 299,  accuracy 99.5995 %\n"
     ]
    }
   ],
   "source": [
    "errors, predictions = train_model(X, y, network, lr=learning_rate, epoches=epoches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epoches), errors)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = np.array([[6,1],[1, 2],[2, 3],[4, 2],[3,2], [7,1], [2, 1],[1,1]])\n",
    "y_set = [1, 0, 0, 1, 1, 1, 1, 0]\n",
    "def predict(test_set, y, network):\n",
    "    \n",
    "    output = [test_set]\n",
    "    predict = []\n",
    "    for layer in range(len(network)):\n",
    "        network[layer].forward(output[-1])\n",
    "        z = network[layer].output\n",
    "        a = activate_sigmoid(z)\n",
    "        output.append(a)\n",
    "    \n",
    "    for i in range(len(output[-1])):\n",
    "#        if output[-1][i] > 0.5:\n",
    "#             predict.append(1)\n",
    "#         else:\n",
    "#             predict.append(0)\n",
    "        predict.append(1) if output[-1][i] > 0.5 else predict.append(0)\n",
    "\n",
    "            \n",
    "        \n",
    "    error = loss_function_mse(output[-1], y)\n",
    "    return output[-1], predict, error\n",
    "\n",
    "output, predict, error = predict(test_set, y_set, network)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected: 1 <==> real: 1\n",
      "expected: 0 <==> real: 0\n",
      "expected: 0 <==> real: 0\n",
      "expected: 1 <==> real: 1\n",
      "expected: 1 <==> real: 1\n",
      "expected: 1 <==> real: 1\n",
      "expected: 1 <==> real: 1\n",
      "expected: 0 <==> real: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predict)):\n",
    "    print(\"expected: {} <==> real: {}\".format(predict[i], y_set[i]))\n",
    "#     print(\"accuracy: {}\".format(max(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python37264bitvenvvirtualenvc8215f58c15a47deba343d80afd29add"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
